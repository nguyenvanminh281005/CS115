{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilf9fjXmgkwN"
      },
      "source": [
        "# Denoising Diffusion Probabilistic Model\n",
        "\n",
        "**Author:** [A_K_Nain](https://twitter.com/A_K_Nain)<br>\n",
        "**Date created:** 2022/11/30<br>\n",
        "**Last modified:** 2022/12/07<br>\n",
        "**Description:** Generating images of flowers with denoising diffusion probabilistic models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrLD-fRJgkwP"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Generative modeling experienced tremendous growth in the last five years. Models like\n",
        "VAEs, GANs, and flow-based models proved to be a great success in generating\n",
        "high-quality content, especially images. Diffusion models are a new type of generative\n",
        "model that has proven to be better than previous approaches.\n",
        "\n",
        "Diffusion models are inspired by non-equilibrium thermodynamics, and they learn to\n",
        "generate by denoising. Learning by denoising consists of two processes,\n",
        "each of which is a Markov Chain. These are:\n",
        "\n",
        "1. The forward process: In the forward process, we slowly add random noise to the data\n",
        "in a series of time steps `(t1, t2, ..., tn )`. Samples at the current time step are\n",
        "drawn from a Gaussian distribution where the mean of the distribution is conditioned\n",
        "on the sample at the previous time step, and the variance of the distribution follows\n",
        "a fixed schedule. At the end of the forward process, the samples end up with a pure\n",
        "noise distribution.\n",
        "\n",
        "2. The reverse process: During the reverse process, we try to undo the added noise at\n",
        "every time step. We start with the pure noise distribution (the last step of the\n",
        "forward process) and try to denoise the samples in the backward direction\n",
        "`(tn, tn-1, ..., t1)`.\n",
        "\n",
        "We implement the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n",
        "paper or DDPMs for short in this code example. It was the first paper demonstrating\n",
        "the use of diffusion models for generating high-quality images. The authors proved\n",
        "that a certain parameterization of diffusion models reveals an equivalence with\n",
        "denoising score matching over multiple noise levels during training and with annealed\n",
        "Langevin dynamics during sampling that generates the best quality results.\n",
        "\n",
        "This paper replicates both the Markov chains (forward process and reverse process)\n",
        "involved in the diffusion process but for images. The forward process is fixed and\n",
        "gradually adds Gaussian noise to the images according to a fixed variance schedule\n",
        "denoted by beta in the paper. This is what the diffusion process looks like in case\n",
        "of images: (image -> noise::noise -> image)\n",
        "\n",
        "![diffusion process gif](https://imgur.com/Yn7tho9.gif)\n",
        "\n",
        "\n",
        "The paper describes two algorithms, one for training the model, and the other for\n",
        "sampling from the trained model. Training is performed by optimizing the usual\n",
        "variational bound on negative log-likelihood. The objective function is further\n",
        "simplified, and the network is treated as a noise prediction network. Once optimized,\n",
        "we can sample from the network to generate new images from noise samples. Here is an\n",
        "overview of both algorithms as presented in the paper:\n",
        "\n",
        "![ddpms](https://i.imgur.com/S7KH5hZ.png)\n",
        "\n",
        "\n",
        "**Note:** DDPM is just one way of implementing a diffusion model. Also, the sampling\n",
        "algorithm in the DDPM replicates the complete Markov chain. Hence, it's slow in\n",
        "generating new samples compared to other generative models like GANs. Lots of research\n",
        "efforts have been made to address this issue. One such example is Denoising Diffusion\n",
        "Implicit Models, or DDIM for short, where the authors replaced the Markov chain with a\n",
        "non-Markovian process to sample faster. You can find the code example for DDIM\n",
        "[here](https://keras.io/examples/generative/ddim/)\n",
        "\n",
        "Implementing a DDPM model is simple. We define a model that takes\n",
        "two inputs: Images and the randomly sampled time steps. At each training step, we\n",
        "perform the following operations to train our model:\n",
        "\n",
        "1. Sample random noise to be added to the inputs.\n",
        "2. Apply the forward process to diffuse the inputs with the sampled noise.\n",
        "3. Your model takes these noisy samples as inputs and outputs the noise\n",
        "prediction for each time step.\n",
        "4. Given true noise and predicted noise, we calculate the loss values\n",
        "5. We then calculate the gradients and update the model weights.\n",
        "\n",
        "Given that our model knows how to denoise a noisy sample at a given time step,\n",
        "we can leverage this idea to generate new samples, starting from a pure noise\n",
        "distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pksuKmPgkwQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WXXSAm9agkwQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Requires TensorFlow >=2.11 for the GroupNormalization layer.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhepgOeBgkwS"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_qCHdbj6gkwT"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "num_epochs = 800  # Just for the sake of demonstration\n",
        "total_timesteps = 1000\n",
        "norm_groups = 8  # Number of groups used in GroupNormalization layer\n",
        "learning_rate = 2e-4\n",
        "\n",
        "img_size = 64\n",
        "img_channels = 3\n",
        "clip_min = -1.0\n",
        "clip_max = 1.0\n",
        "\n",
        "first_conv_channels = 64\n",
        "channel_multiplier = [1, 2, 4, 8]\n",
        "widths = [first_conv_channels * mult for mult in channel_multiplier]\n",
        "has_attention = [False, False, True, True]\n",
        "num_res_blocks = 2  # Number of residual blocks\n",
        "\n",
        "dataset_name = \"oxford_flowers102\"\n",
        "splits = [\"train\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io8KNoyZgkwT"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We use the [Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102)\n",
        "dataset for generating images of flowers. In terms of preprocessing, we use center\n",
        "cropping for resizing the images to the desired image size, and we rescale the pixel\n",
        "values in the range `[-1.0, 1.0]`. This is in line with the range of the pixel values that\n",
        "was applied by the authors of the [DDPMs paper](https://arxiv.org/abs/2006.11239). For\n",
        "augmenting training data, we randomly flip the images left/right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DCBStsDLgkwT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "(ds,) = tfds.load(dataset_name, split=splits, with_info=False, shuffle_files=True)\n",
        "\n",
        "\n",
        "def augment(img):\n",
        "    \"\"\"Flips an image left/right randomly.\"\"\"\n",
        "    return tf.image.random_flip_left_right(img)\n",
        "\n",
        "\n",
        "def resize_and_rescale(img, size):\n",
        "    \"\"\"Resize the image to the desired size first and then\n",
        "    rescale the pixel values in the range [-1.0, 1.0].\n",
        "\n",
        "    Args:\n",
        "        img: Image tensor\n",
        "        size: Desired image size for resizing\n",
        "    Returns:\n",
        "        Resized and rescaled image tensor\n",
        "    \"\"\"\n",
        "\n",
        "    height = tf.shape(img)[0]\n",
        "    width = tf.shape(img)[1]\n",
        "    crop_size = tf.minimum(height, width)\n",
        "\n",
        "    img = tf.image.crop_to_bounding_box(\n",
        "        img,\n",
        "        (height - crop_size) // 2,\n",
        "        (width - crop_size) // 2,\n",
        "        crop_size,\n",
        "        crop_size,\n",
        "    )\n",
        "\n",
        "    # Resize\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    img = tf.image.resize(img, size=size, antialias=True)\n",
        "\n",
        "    # Rescale the pixel values\n",
        "    img = img / 127.5 - 1.0\n",
        "    img = tf.clip_by_value(img, clip_min, clip_max)\n",
        "    return img\n",
        "\n",
        "\n",
        "def train_preprocessing(x):\n",
        "    img = x[\"image\"]\n",
        "    img = resize_and_rescale(img, size=(img_size, img_size))\n",
        "    img = augment(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "train_ds = (\n",
        "    ds.map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(batch_size, drop_remainder=True)\n",
        "    .shuffle(batch_size * 2)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDflqvLwgkwU"
      },
      "source": [
        "## Gaussian diffusion utilities\n",
        "\n",
        "We define the forward process and the reverse process\n",
        "as a separate utility. Most of the code in this utility has been borrowed\n",
        "from the original implementation with some slight modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mD0BO9QzgkwU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GaussianDiffusion:\n",
        "    \"\"\"Gaussian diffusion utility.\n",
        "\n",
        "    Args:\n",
        "        beta_start: Start value of the scheduled variance\n",
        "        beta_end: End value of the scheduled variance\n",
        "        timesteps: Number of time steps in the forward process\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        beta_start=1e-4,\n",
        "        beta_end=0.02,\n",
        "        timesteps=1000,\n",
        "        clip_min=-1.0,\n",
        "        clip_max=1.0,\n",
        "    ):\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        self.timesteps = timesteps\n",
        "        self.clip_min = clip_min\n",
        "        self.clip_max = clip_max\n",
        "\n",
        "        # Define the linear variance schedule\n",
        "        self.betas = betas = np.linspace(\n",
        "            beta_start,\n",
        "            beta_end,\n",
        "            timesteps,\n",
        "            dtype=np.float64,  # Using float64 for better precision\n",
        "        )\n",
        "        self.num_timesteps = int(timesteps)\n",
        "\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
        "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
        "\n",
        "        self.betas = tf.constant(betas, dtype=tf.float32)\n",
        "        self.alphas_cumprod = tf.constant(alphas_cumprod, dtype=tf.float32)\n",
        "        self.alphas_cumprod_prev = tf.constant(alphas_cumprod_prev, dtype=tf.float32)\n",
        "\n",
        "        # Calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.sqrt_alphas_cumprod = tf.constant(\n",
        "            np.sqrt(alphas_cumprod), dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        self.sqrt_one_minus_alphas_cumprod = tf.constant(\n",
        "            np.sqrt(1.0 - alphas_cumprod), dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        self.log_one_minus_alphas_cumprod = tf.constant(\n",
        "            np.log(1.0 - alphas_cumprod), dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        self.sqrt_recip_alphas_cumprod = tf.constant(\n",
        "            np.sqrt(1.0 / alphas_cumprod), dtype=tf.float32\n",
        "        )\n",
        "        self.sqrt_recipm1_alphas_cumprod = tf.constant(\n",
        "            np.sqrt(1.0 / alphas_cumprod - 1), dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        posterior_variance = (\n",
        "            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "        )\n",
        "        self.posterior_variance = tf.constant(posterior_variance, dtype=tf.float32)\n",
        "\n",
        "        # Log calculation clipped because the posterior variance is 0 at the beginning\n",
        "        # of the diffusion chain\n",
        "        self.posterior_log_variance_clipped = tf.constant(\n",
        "            np.log(np.maximum(posterior_variance, 1e-20)), dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        self.posterior_mean_coef1 = tf.constant(\n",
        "            betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "        self.posterior_mean_coef2 = tf.constant(\n",
        "            (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod),\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "    def _extract(self, a, t, x_shape):\n",
        "        \"\"\"Extract some coefficients at specified timesteps,\n",
        "        then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
        "\n",
        "        Args:\n",
        "            a: Tensor to extract from\n",
        "            t: Timestep for which the coefficients are to be extracted\n",
        "            x_shape: Shape of the current batched samples\n",
        "        \"\"\"\n",
        "        batch_size = x_shape[0]\n",
        "        out = tf.gather(a, t)\n",
        "        return tf.reshape(out, [batch_size, 1, 1, 1])\n",
        "\n",
        "    def q_mean_variance(self, x_start, t):\n",
        "        \"\"\"Extracts the mean, and the variance at current timestep.\n",
        "\n",
        "        Args:\n",
        "            x_start: Initial sample (before the first diffusion step)\n",
        "            t: Current timestep\n",
        "        \"\"\"\n",
        "        x_start_shape = tf.shape(x_start)\n",
        "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start_shape) * x_start\n",
        "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start_shape)\n",
        "        log_variance = self._extract(\n",
        "            self.log_one_minus_alphas_cumprod, t, x_start_shape\n",
        "        )\n",
        "        return mean, variance, log_variance\n",
        "\n",
        "    def q_sample(self, x_start, t, noise):\n",
        "        \"\"\"Diffuse the data.\n",
        "\n",
        "        Args:\n",
        "            x_start: Initial sample (before the first diffusion step)\n",
        "            t: Current timestep\n",
        "            noise: Gaussian noise to be added at the current timestep\n",
        "        Returns:\n",
        "            Diffused samples at timestep `t`\n",
        "        \"\"\"\n",
        "        x_start_shape = tf.shape(x_start)\n",
        "        return (\n",
        "            self._extract(self.sqrt_alphas_cumprod, t, tf.shape(x_start)) * x_start\n",
        "            + self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start_shape)\n",
        "            * noise\n",
        "        )\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        x_t_shape = tf.shape(x_t)\n",
        "        return (\n",
        "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t_shape) * x_t\n",
        "            - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t_shape) * noise\n",
        "        )\n",
        "\n",
        "    def q_posterior(self, x_start, x_t, t):\n",
        "        \"\"\"Compute the mean and variance of the diffusion\n",
        "        posterior q(x_{t-1} | x_t, x_0).\n",
        "\n",
        "        Args:\n",
        "            x_start: Stating point(sample) for the posterior computation\n",
        "            x_t: Sample at timestep `t`\n",
        "            t: Current timestep\n",
        "        Returns:\n",
        "            Posterior mean and variance at current timestep\n",
        "        \"\"\"\n",
        "\n",
        "        x_t_shape = tf.shape(x_t)\n",
        "        posterior_mean = (\n",
        "            self._extract(self.posterior_mean_coef1, t, x_t_shape) * x_start\n",
        "            + self._extract(self.posterior_mean_coef2, t, x_t_shape) * x_t\n",
        "        )\n",
        "        posterior_variance = self._extract(self.posterior_variance, t, x_t_shape)\n",
        "        posterior_log_variance_clipped = self._extract(\n",
        "            self.posterior_log_variance_clipped, t, x_t_shape\n",
        "        )\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    def p_mean_variance(self, pred_noise, x, t, clip_denoised=True):\n",
        "        x_recon = self.predict_start_from_noise(x, t=t, noise=pred_noise)\n",
        "        if clip_denoised:\n",
        "            x_recon = tf.clip_by_value(x_recon, self.clip_min, self.clip_max)\n",
        "\n",
        "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n",
        "            x_start=x_recon, x_t=x, t=t\n",
        "        )\n",
        "        return model_mean, posterior_variance, posterior_log_variance\n",
        "\n",
        "    def p_sample(self, pred_noise, x, t, clip_denoised=True):\n",
        "        \"\"\"Sample from the diffusion model.\n",
        "\n",
        "        Args:\n",
        "            pred_noise: Noise predicted by the diffusion model\n",
        "            x: Samples at a given timestep for which the noise was predicted\n",
        "            t: Current timestep\n",
        "            clip_denoised (bool): Whether to clip the predicted noise\n",
        "                within the specified range or not.\n",
        "        \"\"\"\n",
        "        model_mean, _, model_log_variance = self.p_mean_variance(\n",
        "            pred_noise, x=x, t=t, clip_denoised=clip_denoised\n",
        "        )\n",
        "        noise = tf.random.normal(shape=x.shape, dtype=x.dtype)\n",
        "        # No noise when t == 0\n",
        "        nonzero_mask = tf.reshape(\n",
        "            1 - tf.cast(tf.equal(t, 0), tf.float32), [tf.shape(x)[0], 1, 1, 1]\n",
        "        )\n",
        "        return model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJOrSZL2gkwU"
      },
      "source": [
        "## Network architecture\n",
        "\n",
        "U-Net, originally developed for semantic segmentation, is an architecture that is\n",
        "widely used for implementing diffusion models but with some slight modifications:\n",
        "\n",
        "1. The network accepts two inputs: Image and time step\n",
        "2. Self-attention between the convolution blocks once we reach a specific resolution\n",
        "(16x16 in the paper)\n",
        "3. Group Normalization instead of weight normalization\n",
        "\n",
        "We implement most of the things as used in the original paper. We use the\n",
        "`swish` activation function throughout the network. We use the variance scaling\n",
        "kernel initializer.\n",
        "\n",
        "The only difference here is the number of groups used for the\n",
        "`GroupNormalization` layer. For the flowers dataset,\n",
        "we found that a value of `groups=8` produces better results\n",
        "compared to the default value of `groups=32`. Dropout is optional and should be\n",
        "used where chances of over fitting is high. In the paper, the authors used dropout\n",
        "only when training on CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vMj4rpTJgkwU"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "# Kernel initializer to use\n",
        "def kernel_init(scale):\n",
        "    scale = max(scale, 1e-10)\n",
        "    return keras.initializers.VarianceScaling(\n",
        "        scale, mode=\"fan_avg\", distribution=\"uniform\"\n",
        "    )\n",
        "\n",
        "\n",
        "class AttentionBlock(layers.Layer):\n",
        "    \"\"\"Applies self-attention.\n",
        "\n",
        "    Args:\n",
        "        units: Number of units in the dense layers\n",
        "        groups: Number of groups to be used for GroupNormalization layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, groups=8, **kwargs):\n",
        "        self.units = units\n",
        "        self.groups = groups\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.norm = tfa.layers.GroupNormalization(groups=groups)\n",
        "        self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n",
        "        self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n",
        "        self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0))\n",
        "        self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height = tf.shape(inputs)[1]\n",
        "        width = tf.shape(inputs)[2]\n",
        "        scale = tf.cast(self.units, tf.float32) ** (-0.5)\n",
        "\n",
        "        inputs = self.norm(inputs)\n",
        "        q = self.query(inputs)\n",
        "        k = self.key(inputs)\n",
        "        v = self.value(inputs)\n",
        "\n",
        "        attn_score = tf.einsum(\"bhwc, bHWc->bhwHW\", q, k) * scale\n",
        "        attn_score = tf.reshape(attn_score, [batch_size, height, width, height * width])\n",
        "\n",
        "        attn_score = tf.nn.softmax(attn_score, -1)\n",
        "        attn_score = tf.reshape(attn_score, [batch_size, height, width, height, width])\n",
        "\n",
        "        proj = tf.einsum(\"bhwHW,bHWc->bhwc\", attn_score, v)\n",
        "        proj = self.proj(proj)\n",
        "        return inputs + proj\n",
        "\n",
        "\n",
        "class TimeEmbedding(layers.Layer):\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.half_dim = dim // 2\n",
        "        self.emb = math.log(10000) / (self.half_dim - 1)\n",
        "        self.emb = tf.exp(tf.range(self.half_dim, dtype=tf.float32) * -self.emb)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
        "        emb = inputs[:, None] * self.emb[None, :]\n",
        "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish):\n",
        "    def apply(inputs):\n",
        "        x, t = inputs\n",
        "        input_width = x.shape[3]\n",
        "\n",
        "        if input_width == width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual = layers.Conv2D(\n",
        "                width, kernel_size=1, kernel_initializer=kernel_init(1.0)\n",
        "            )(x)\n",
        "\n",
        "        temb = activation_fn(t)\n",
        "        temb = layers.Dense(width, kernel_initializer=kernel_init(1.0))(temb)[\n",
        "            :, None, None, :\n",
        "        ]\n",
        "\n",
        "        x = tfa.layers.GroupNormalization(groups=groups)(x)\n",
        "        x = activation_fn(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(1.0)\n",
        "        )(x)\n",
        "\n",
        "        x = layers.Add()([x, temb])\n",
        "        x = tfa.layers.GroupNormalization(groups=groups)(x)\n",
        "        x = activation_fn(x)\n",
        "\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(0.0)\n",
        "        )(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def DownSample(width):\n",
        "    def apply(x):\n",
        "        x = layers.Conv2D(\n",
        "            width,\n",
        "            kernel_size=3,\n",
        "            strides=2,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init(1.0),\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def UpSample(width, interpolation=\"nearest\"):\n",
        "    def apply(x):\n",
        "        x = layers.UpSampling2D(size=2, interpolation=interpolation)(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(1.0)\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def TimeMLP(units, activation_fn=keras.activations.swish):\n",
        "    def apply(inputs):\n",
        "        temb = layers.Dense(\n",
        "            units, activation=activation_fn, kernel_initializer=kernel_init(1.0)\n",
        "        )(inputs)\n",
        "        temb = layers.Dense(units, kernel_initializer=kernel_init(1.0))(temb)\n",
        "        return temb\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    img_size,\n",
        "    img_channels,\n",
        "    widths,\n",
        "    has_attention,\n",
        "    num_res_blocks=2,\n",
        "    norm_groups=8,\n",
        "    interpolation=\"nearest\",\n",
        "    activation_fn=keras.activations.swish,\n",
        "):\n",
        "    image_input = layers.Input(\n",
        "        shape=(img_size, img_size, img_channels), name=\"image_input\"\n",
        "    )\n",
        "    time_input = keras.Input(shape=(), dtype=tf.int64, name=\"time_input\")\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        first_conv_channels,\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_init(1.0),\n",
        "    )(image_input)\n",
        "\n",
        "    temb = TimeEmbedding(dim=first_conv_channels * 4)(time_input)\n",
        "    temb = TimeMLP(units=first_conv_channels * 4, activation_fn=activation_fn)(temb)\n",
        "\n",
        "    skips = [x]\n",
        "\n",
        "    # DownBlock\n",
        "    for i in range(len(widths)):\n",
        "        for _ in range(num_res_blocks):\n",
        "            x = ResidualBlock(\n",
        "                widths[i], groups=norm_groups, activation_fn=activation_fn\n",
        "            )([x, temb])\n",
        "            if has_attention[i]:\n",
        "                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        if widths[i] != widths[-1]:\n",
        "            x = DownSample(widths[i])(x)\n",
        "            skips.append(x)\n",
        "\n",
        "    # MiddleBlock\n",
        "    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn)(\n",
        "        [x, temb]\n",
        "    )\n",
        "    x = AttentionBlock(widths[-1], groups=norm_groups)(x)\n",
        "    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn)(\n",
        "        [x, temb]\n",
        "    )\n",
        "\n",
        "    # UpBlock\n",
        "    for i in reversed(range(len(widths))):\n",
        "        for _ in range(num_res_blocks + 1):\n",
        "            x = layers.Concatenate(axis=-1)([x, skips.pop()])\n",
        "            x = ResidualBlock(\n",
        "                widths[i], groups=norm_groups, activation_fn=activation_fn\n",
        "            )([x, temb])\n",
        "            if has_attention[i]:\n",
        "                x = AttentionBlock(widths[i], groups=norm_groups)(x)\n",
        "\n",
        "        if i != 0:\n",
        "            x = UpSample(widths[i], interpolation=interpolation)(x)\n",
        "\n",
        "    # End block\n",
        "    x = tfa.layers.GroupNormalization(groups=norm_groups)(x)\n",
        "    x = activation_fn(x)\n",
        "    x = layers.Conv2D(3, (3, 3), padding=\"same\", kernel_initializer=kernel_init(0.0))(x)\n",
        "    return keras.Model([image_input, time_input], x, name=\"unet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdhD_qU6gkwV"
      },
      "source": [
        "## Training\n",
        "\n",
        "We follow the same setup for training the diffusion model as described\n",
        "in the paper. We use `Adam` optimizer with a learning rate of `2e-4`.\n",
        "We use EMA on model parameters with a decay factor of 0.999. We\n",
        "treat our model as noise prediction network i.e. at every training step, we\n",
        "input a batch of images and corresponding time steps to our UNet,\n",
        "and the network outputs the noise as predictions.\n",
        "\n",
        "The only difference is that we aren't using the Kernel Inception Distance (KID)\n",
        "or Frechet Inception Distance (FID) for evaluating the quality of generated\n",
        "samples during training. This is because both these metrics are compute heavy\n",
        "and are skipped for the brevity of implementation.\n",
        "\n",
        "**Note: ** We are using mean squared error as the loss function which is aligned with\n",
        "the paper, and theoretically makes sense. In practice, though, it is also common to\n",
        "use mean absolute error or Huber loss as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vsweHcTQgkwV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5339Epoch 1, Loss: 0.151931494474411\n",
            "63/63 [==============================] - 48s 374ms/step - loss: 0.5279\n",
            "Epoch 2/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0998Epoch 2, Loss: 0.10100558400154114\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0998\n",
            "Epoch 3/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0626Epoch 3, Loss: 0.028004948049783707\n",
            "63/63 [==============================] - 24s 368ms/step - loss: 0.0620\n",
            "Epoch 4/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0530Epoch 4, Loss: 0.05305144935846329\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0530\n",
            "Epoch 5/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0511Epoch 5, Loss: 0.054634302854537964\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0511\n",
            "Epoch 6/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0445Epoch 6, Loss: 0.01658371090888977\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0441\n",
            "Epoch 7/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0463Epoch 7, Loss: 0.0461360365152359\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0463\n",
            "Epoch 8/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0406Epoch 8, Loss: 0.054166391491889954\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0408\n",
            "Epoch 9/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0451Epoch 9, Loss: 0.042543552815914154\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0451\n",
            "Epoch 10/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0457Epoch 10, Loss: 0.0431286096572876\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0457\n",
            "Epoch 11/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0434Epoch 11, Loss: 0.05704357102513313\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0436\n",
            "Epoch 12/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0384Epoch 12, Loss: 0.04275742918252945\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0385\n",
            "Epoch 13/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0346Epoch 13, Loss: 0.05417554825544357\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0349\n",
            "Epoch 14/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0360Epoch 14, Loss: 0.018141034990549088\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0357\n",
            "Epoch 15/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0407Epoch 15, Loss: 0.04341479018330574\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0407\n",
            "Epoch 16/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0420Epoch 16, Loss: 0.02861553244292736\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0418\n",
            "Epoch 17/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0458Epoch 17, Loss: 0.017903152853250504\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0453\n",
            "Epoch 18/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0423Epoch 18, Loss: 0.047291629016399384\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0424\n",
            "Epoch 19/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0394Epoch 19, Loss: 0.020134681835770607\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0391\n",
            "Epoch 20/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0384Epoch 20, Loss: 0.06133236736059189\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0388\n",
            "Epoch 21/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0325Epoch 21, Loss: 0.05609612166881561\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0329\n",
            "Epoch 22/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0397Epoch 22, Loss: 0.019801868125796318\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0394\n",
            "Epoch 23/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0353Epoch 23, Loss: 0.03092971071600914\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0352\n",
            "Epoch 24/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0374Epoch 24, Loss: 0.08581719547510147\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0382\n",
            "Epoch 25/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0351Epoch 25, Loss: 0.01688079535961151\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0348\n",
            "Epoch 26/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0366Epoch 26, Loss: 0.022850025445222855\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0364\n",
            "Epoch 27/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0385Epoch 27, Loss: 0.04608093574643135\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0386\n",
            "Epoch 28/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0353Epoch 28, Loss: 0.02577310800552368\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0352\n",
            "Epoch 29/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0380Epoch 29, Loss: 0.031160425394773483\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0379\n",
            "Epoch 30/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0344Epoch 30, Loss: 0.02089572325348854\n",
            "63/63 [==============================] - 26s 405ms/step - loss: 0.0342\n",
            "Epoch 31/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0353Epoch 31, Loss: 0.050553929060697556\n",
            "63/63 [==============================] - 33s 497ms/step - loss: 0.0355\n",
            "Epoch 32/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0358Epoch 32, Loss: 0.025776483118534088\n",
            "63/63 [==============================] - 35s 531ms/step - loss: 0.0356\n",
            "Epoch 33/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0339Epoch 33, Loss: 0.05120183154940605\n",
            "63/63 [==============================] - 26s 380ms/step - loss: 0.0341\n",
            "Epoch 34/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0376Epoch 34, Loss: 0.0710996687412262\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0381\n",
            "Epoch 35/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0321Epoch 35, Loss: 0.011414706707000732\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0318\n",
            "Epoch 36/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 36, Loss: 0.01496998593211174\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0315\n",
            "Epoch 37/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0360Epoch 37, Loss: 0.019867513328790665\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0358\n",
            "Epoch 38/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0323Epoch 38, Loss: 0.01829536072909832\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0321\n",
            "Epoch 39/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0352Epoch 39, Loss: 0.038192979991436005\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0352\n",
            "Epoch 40/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0341Epoch 40, Loss: 0.024088416248559952\n",
            "63/63 [==============================] - 27s 430ms/step - loss: 0.0339\n",
            "Epoch 41/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0321Epoch 41, Loss: 0.02124723419547081\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0320\n",
            "Epoch 42/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0358Epoch 42, Loss: 0.023753952234983444\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0356\n",
            "Epoch 43/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0336Epoch 43, Loss: 0.01906363107264042\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0334\n",
            "Epoch 44/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 44, Loss: 0.03368314355611801\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0337\n",
            "Epoch 45/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 45, Loss: 0.029621314257383347\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0325\n",
            "Epoch 46/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0362Epoch 46, Loss: 0.02568851225078106\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0361\n",
            "Epoch 47/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0366Epoch 47, Loss: 0.0473526231944561\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0367\n",
            "Epoch 48/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0331Epoch 48, Loss: 0.02339349128305912\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0330\n",
            "Epoch 49/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 49, Loss: 0.016022473573684692\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0311\n",
            "Epoch 50/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0333Epoch 50, Loss: 0.054833609610795975\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0336\n",
            "Epoch 51/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0365Epoch 51, Loss: 0.019769366830587387\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0362\n",
            "Epoch 52/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0363Epoch 52, Loss: 0.046336665749549866\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0365\n",
            "Epoch 53/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 53, Loss: 0.027600225061178207\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0301\n",
            "Epoch 54/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0350Epoch 54, Loss: 0.009925603866577148\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0346\n",
            "Epoch 55/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0334Epoch 55, Loss: 0.03540898859500885\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0334\n",
            "Epoch 56/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 56, Loss: 0.02109282836318016\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0302\n",
            "Epoch 57/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0342Epoch 57, Loss: 0.041179217398166656\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0343\n",
            "Epoch 58/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0323Epoch 58, Loss: 0.007109102793037891\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0319\n",
            "Epoch 59/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0365Epoch 59, Loss: 0.027708150446414948\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0363\n",
            "Epoch 60/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 60, Loss: 0.00608318205922842\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0321\n",
            "Epoch 61/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 61, Loss: 0.05545417591929436\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0333\n",
            "Epoch 62/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 62, Loss: 0.04429870471358299\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0308\n",
            "Epoch 63/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0362Epoch 63, Loss: 0.009293090552091599\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0357\n",
            "Epoch 64/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 64, Loss: 0.019117895513772964\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0290\n",
            "Epoch 65/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0336Epoch 65, Loss: 0.029180999845266342\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0335\n",
            "Epoch 66/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 66, Loss: 0.04760858416557312\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0304\n",
            "Epoch 67/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 67, Loss: 0.026372848078608513\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0336\n",
            "Epoch 68/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0373Epoch 68, Loss: 0.033450864255428314\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0372\n",
            "Epoch 69/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0354Epoch 69, Loss: 0.03556019812822342\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0354\n",
            "Epoch 70/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0323Epoch 70, Loss: 0.01727229915559292\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0321\n",
            "Epoch 71/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0355Epoch 71, Loss: 0.08185901492834091\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0363\n",
            "Epoch 72/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0340Epoch 72, Loss: 0.01971765048801899\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0337\n",
            "Epoch 73/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0383Epoch 73, Loss: 0.049500107765197754\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0385\n",
            "Epoch 74/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 74, Loss: 0.028586890548467636\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0318\n",
            "Epoch 75/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 75, Loss: 0.013576153665781021\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0268\n",
            "Epoch 76/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 76, Loss: 0.0463644303381443\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0318\n",
            "Epoch 77/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0348Epoch 77, Loss: 0.06532974541187286\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0353\n",
            "Epoch 78/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 78, Loss: 0.058962635695934296\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0321\n",
            "Epoch 79/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0304Epoch 79, Loss: 0.024007335305213928\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0303\n",
            "Epoch 80/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 80, Loss: 0.016918715089559555\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0308\n",
            "Epoch 81/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 81, Loss: 0.010617030784487724\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0333\n",
            "Epoch 82/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0340Epoch 82, Loss: 0.048015251755714417\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0343\n",
            "Epoch 83/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0312Epoch 83, Loss: 0.046218689531087875\n",
            "63/63 [==============================] - 24s 384ms/step - loss: 0.0314\n",
            "Epoch 84/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0357Epoch 84, Loss: 0.031693294644355774\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0356\n",
            "Epoch 85/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0348Epoch 85, Loss: 0.011117030866444111\n",
            "63/63 [==============================] - 25s 389ms/step - loss: 0.0344\n",
            "Epoch 86/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0330Epoch 86, Loss: 0.0143973957747221\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0327\n",
            "Epoch 87/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0320Epoch 87, Loss: 0.028107086196541786\n",
            "63/63 [==============================] - 25s 387ms/step - loss: 0.0319\n",
            "Epoch 88/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 88, Loss: 0.011940261349081993\n",
            "63/63 [==============================] - 25s 387ms/step - loss: 0.0277\n",
            "Epoch 89/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0346Epoch 89, Loss: 0.021889306604862213\n",
            "63/63 [==============================] - 25s 387ms/step - loss: 0.0344\n",
            "Epoch 90/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 90, Loss: 0.04080886393785477\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0292\n",
            "Epoch 91/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0330Epoch 91, Loss: 0.008829747326672077\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0326\n",
            "Epoch 92/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 92, Loss: 0.024378295987844467\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0313\n",
            "Epoch 93/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 93, Loss: 0.017522569745779037\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0305\n",
            "Epoch 94/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 94, Loss: 0.01766830123960972\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0316\n",
            "Epoch 95/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0363Epoch 95, Loss: 0.058199070394039154\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0366\n",
            "Epoch 96/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 96, Loss: 0.031138665974140167\n",
            "63/63 [==============================] - 24s 384ms/step - loss: 0.0336\n",
            "Epoch 97/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 97, Loss: 0.008929051458835602\n",
            "63/63 [==============================] - 25s 388ms/step - loss: 0.0308\n",
            "Epoch 98/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 98, Loss: 0.020232684910297394\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0326\n",
            "Epoch 99/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 99, Loss: 0.02106131799519062\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0274\n",
            "Epoch 100/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0330Epoch 100, Loss: 0.05414837598800659\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0333\n",
            "Epoch 101/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0344Epoch 101, Loss: 0.014831025153398514\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0340\n",
            "Epoch 102/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 102, Loss: 0.029279155656695366\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0311\n",
            "Epoch 103/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0341Epoch 103, Loss: 0.013455642387270927\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0337\n",
            "Epoch 104/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 104, Loss: 0.054083794355392456\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0297\n",
            "Epoch 105/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 105, Loss: 0.01948593556880951\n",
            "63/63 [==============================] - 25s 388ms/step - loss: 0.0300\n",
            "Epoch 106/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0304Epoch 106, Loss: 0.03212043270468712\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0305\n",
            "Epoch 107/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0321Epoch 107, Loss: 0.0207159835845232\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0320\n",
            "Epoch 108/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 108, Loss: 0.022604260593652725\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0318\n",
            "Epoch 109/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 109, Loss: 0.03099196031689644\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0317\n",
            "Epoch 110/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 110, Loss: 0.04671072959899902\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0331\n",
            "Epoch 111/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0336Epoch 111, Loss: 0.041470155119895935\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0337\n",
            "Epoch 112/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 112, Loss: 0.021662138402462006\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0312\n",
            "Epoch 113/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0332Epoch 113, Loss: 0.03706274554133415\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0333\n",
            "Epoch 114/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 114, Loss: 0.0514737032353878\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0319\n",
            "Epoch 115/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0344Epoch 115, Loss: 0.016060085967183113\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0342\n",
            "Epoch 116/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 116, Loss: 0.03094240091741085\n",
            "63/63 [==============================] - 25s 387ms/step - loss: 0.0282\n",
            "Epoch 117/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0351Epoch 117, Loss: 0.02864367514848709\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0350\n",
            "Epoch 118/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0334Epoch 118, Loss: 0.0704381912946701\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0340\n",
            "Epoch 119/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 119, Loss: 0.014090370386838913\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0286\n",
            "Epoch 120/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 120, Loss: 0.07362457364797592\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0320\n",
            "Epoch 121/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0325Epoch 121, Loss: 0.04090921953320503\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0327\n",
            "Epoch 122/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0341Epoch 122, Loss: 0.01812589354813099\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0339\n",
            "Epoch 123/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 123, Loss: 0.012784521095454693\n",
            "63/63 [==============================] - 25s 390ms/step - loss: 0.0287\n",
            "Epoch 124/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 124, Loss: 0.02649173140525818\n",
            "63/63 [==============================] - 25s 384ms/step - loss: 0.0312\n",
            "Epoch 125/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 125, Loss: 0.032138850539922714\n",
            "63/63 [==============================] - 25s 384ms/step - loss: 0.0292\n",
            "Epoch 126/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0339Epoch 126, Loss: 0.016285600140690804\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0336\n",
            "Epoch 127/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0342Epoch 127, Loss: 0.0229977797716856\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0340\n",
            "Epoch 128/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 128, Loss: 0.022793471813201904\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0311\n",
            "Epoch 129/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 129, Loss: 0.01942165195941925\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0290\n",
            "Epoch 130/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0324Epoch 130, Loss: 0.013507735915482044\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0321\n",
            "Epoch 131/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 131, Loss: 0.006157999392598867\n",
            "63/63 [==============================] - 28s 440ms/step - loss: 0.0322\n",
            "Epoch 132/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0358Epoch 132, Loss: 0.06718742102384567\n",
            "63/63 [==============================] - 35s 536ms/step - loss: 0.0363\n",
            "Epoch 133/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 133, Loss: 0.03672882542014122\n",
            "63/63 [==============================] - 32s 489ms/step - loss: 0.0312\n",
            "Epoch 134/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 134, Loss: 0.02724081464111805\n",
            "63/63 [==============================] - 30s 462ms/step - loss: 0.0293\n",
            "Epoch 135/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 135, Loss: 0.031090805307030678\n",
            "63/63 [==============================] - 33s 504ms/step - loss: 0.0311\n",
            "Epoch 136/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0323Epoch 136, Loss: 0.017918657511472702\n",
            "63/63 [==============================] - 35s 527ms/step - loss: 0.0320\n",
            "Epoch 137/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 137, Loss: 0.023856278508901596\n",
            "63/63 [==============================] - 33s 501ms/step - loss: 0.0304\n",
            "Epoch 138/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 138, Loss: 0.025224629789590836\n",
            "63/63 [==============================] - 34s 509ms/step - loss: 0.0315\n",
            "Epoch 139/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 139, Loss: 0.02796807885169983\n",
            "63/63 [==============================] - 33s 499ms/step - loss: 0.0302\n",
            "Epoch 140/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 140, Loss: 0.010906036011874676\n",
            "63/63 [==============================] - 31s 469ms/step - loss: 0.0325\n",
            "Epoch 141/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0322Epoch 141, Loss: 0.004298840183764696\n",
            "63/63 [==============================] - 27s 406ms/step - loss: 0.0318\n",
            "Epoch 142/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 142, Loss: 0.0768512487411499\n",
            "63/63 [==============================] - 27s 407ms/step - loss: 0.0308\n",
            "Epoch 143/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 143, Loss: 0.023239046335220337\n",
            "63/63 [==============================] - 28s 416ms/step - loss: 0.0293\n",
            "Epoch 144/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 144, Loss: 0.0345933772623539\n",
            "63/63 [==============================] - 28s 421ms/step - loss: 0.0286\n",
            "Epoch 145/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 145, Loss: 0.05239786207675934\n",
            "63/63 [==============================] - 24s 384ms/step - loss: 0.0314\n",
            "Epoch 146/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 146, Loss: 0.025988299399614334\n",
            "63/63 [==============================] - 28s 428ms/step - loss: 0.0302\n",
            "Epoch 147/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 147, Loss: 0.025436127558350563\n",
            "63/63 [==============================] - 29s 442ms/step - loss: 0.0316\n",
            "Epoch 148/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 148, Loss: 0.032389912754297256\n",
            "63/63 [==============================] - 30s 449ms/step - loss: 0.0312\n",
            "Epoch 149/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0325Epoch 149, Loss: 0.03604204207658768\n",
            "63/63 [==============================] - 29s 445ms/step - loss: 0.0325\n",
            "Epoch 150/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 150, Loss: 0.01903494819998741\n",
            "63/63 [==============================] - 29s 441ms/step - loss: 0.0306\n",
            "Epoch 151/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 151, Loss: 0.010927435010671616\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0315\n",
            "Epoch 152/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 152, Loss: 0.035605739802122116\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0302\n",
            "Epoch 153/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 153, Loss: 0.044346824288368225\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0298\n",
            "Epoch 154/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 154, Loss: 0.030338265001773834\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0264\n",
            "Epoch 155/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0324Epoch 155, Loss: 0.01979800872504711\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0322\n",
            "Epoch 156/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 156, Loss: 0.011468032374978065\n",
            "63/63 [==============================] - 25s 384ms/step - loss: 0.0333\n",
            "Epoch 157/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 157, Loss: 0.011998741887509823\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0272\n",
            "Epoch 158/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0259Epoch 158, Loss: 0.030641082674264908\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0260\n",
            "Epoch 159/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0312Epoch 159, Loss: 0.05469537153840065\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0315\n",
            "Epoch 160/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0320Epoch 160, Loss: 0.061653293669223785\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0325\n",
            "Epoch 161/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 161, Loss: 0.013600409962236881\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0312\n",
            "Epoch 162/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 162, Loss: 0.014293184503912926\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0323\n",
            "Epoch 163/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0341Epoch 163, Loss: 0.03889303281903267\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0342\n",
            "Epoch 164/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 164, Loss: 0.018488463014364243\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0273\n",
            "Epoch 165/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 165, Loss: 0.037851810455322266\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0306\n",
            "Epoch 166/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 166, Loss: 0.038895487785339355\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0320\n",
            "Epoch 167/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 167, Loss: 0.030754033476114273\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0306\n",
            "Epoch 168/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 168, Loss: 0.03749813884496689\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0318\n",
            "Epoch 169/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 169, Loss: 0.04657400771975517\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0317\n",
            "Epoch 170/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0355Epoch 170, Loss: 0.017866095528006554\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0352\n",
            "Epoch 171/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 171, Loss: 0.021565405651926994\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0267\n",
            "Epoch 172/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 172, Loss: 0.08087368309497833\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0336\n",
            "Epoch 173/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 173, Loss: 0.021927621215581894\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0313\n",
            "Epoch 174/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0332Epoch 174, Loss: 0.030126290395855904\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0332\n",
            "Epoch 175/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 175, Loss: 0.02094198390841484\n",
            "63/63 [==============================] - 24s 384ms/step - loss: 0.0295\n",
            "Epoch 176/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 176, Loss: 0.01907406561076641\n",
            "63/63 [==============================] - 25s 382ms/step - loss: 0.0301\n",
            "Epoch 177/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0310Epoch 177, Loss: 0.059025272727012634\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0315\n",
            "Epoch 178/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 178, Loss: 0.04179474711418152\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0301\n",
            "Epoch 179/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 179, Loss: 0.014172039926052094\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0311\n",
            "Epoch 180/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 180, Loss: 0.02526817098259926\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0315\n",
            "Epoch 181/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 181, Loss: 0.04045472666621208\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0330\n",
            "Epoch 182/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 182, Loss: 0.030705703422427177\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0288\n",
            "Epoch 183/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 183, Loss: 0.025700509548187256\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0290\n",
            "Epoch 184/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 184, Loss: 0.035434480756521225\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0294\n",
            "Epoch 185/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0325Epoch 185, Loss: 0.04814792796969414\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0328\n",
            "Epoch 186/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0321Epoch 186, Loss: 0.021502412855625153\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0319\n",
            "Epoch 187/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0345Epoch 187, Loss: 0.03954757750034332\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0345\n",
            "Epoch 188/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 188, Loss: 0.011284607462584972\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0292\n",
            "Epoch 189/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0330Epoch 189, Loss: 0.0471925288438797\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0332\n",
            "Epoch 190/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 190, Loss: 0.015203412622213364\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0268\n",
            "Epoch 191/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 191, Loss: 0.0318894237279892\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0317\n",
            "Epoch 192/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0278Epoch 192, Loss: 0.043394554406404495\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0280\n",
            "Epoch 193/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 193, Loss: 0.034379102289676666\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0319\n",
            "Epoch 194/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 194, Loss: 0.014013854786753654\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0290\n",
            "Epoch 195/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 195, Loss: 0.019360970705747604\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0335\n",
            "Epoch 196/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0254Epoch 196, Loss: 0.04224685579538345\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0257\n",
            "Epoch 197/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 197, Loss: 0.0806080624461174\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0322\n",
            "Epoch 198/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 198, Loss: 0.016036324203014374\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0325\n",
            "Epoch 199/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0327Epoch 199, Loss: 0.01864195056259632\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0325\n",
            "Epoch 200/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 200, Loss: 0.013143263757228851\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0288\n",
            "Epoch 201/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0324Epoch 201, Loss: 0.030002696439623833\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0323\n",
            "Epoch 202/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 202, Loss: 0.010421114042401314\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0297\n",
            "Epoch 203/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0304Epoch 203, Loss: 0.018346887081861496\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0302\n",
            "Epoch 204/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 204, Loss: 0.014123979024589062\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0302\n",
            "Epoch 205/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 205, Loss: 0.025045447051525116\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0294\n",
            "Epoch 206/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 206, Loss: 0.021588897332549095\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0304\n",
            "Epoch 207/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 207, Loss: 0.0861554965376854\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0337\n",
            "Epoch 208/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 208, Loss: 0.04314813017845154\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0296\n",
            "Epoch 209/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 209, Loss: 0.02140427939593792\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0300\n",
            "Epoch 210/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 210, Loss: 0.034778397530317307\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0268\n",
            "Epoch 211/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 211, Loss: 0.016651198267936707\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0298\n",
            "Epoch 212/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 212, Loss: 0.02572423592209816\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0310\n",
            "Epoch 213/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 213, Loss: 0.025134023278951645\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0307\n",
            "Epoch 214/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0337Epoch 214, Loss: 0.031044913455843925\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0337\n",
            "Epoch 215/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 215, Loss: 0.03831271454691887\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0301\n",
            "Epoch 216/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 216, Loss: 0.031850989907979965\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0328\n",
            "Epoch 217/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0345Epoch 217, Loss: 0.026169339194893837\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0344\n",
            "Epoch 218/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 218, Loss: 0.015209744684398174\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0291\n",
            "Epoch 219/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 219, Loss: 0.07639108598232269\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0294\n",
            "Epoch 220/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 220, Loss: 0.00915744248777628\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0314\n",
            "Epoch 221/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 221, Loss: 0.023601949214935303\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0291\n",
            "Epoch 222/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 222, Loss: 0.04153286665678024\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0297\n",
            "Epoch 223/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 223, Loss: 0.055371467024087906\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0305\n",
            "Epoch 224/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 224, Loss: 0.029219333082437515\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0290\n",
            "Epoch 225/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0334Epoch 225, Loss: 0.024889029562473297\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0333\n",
            "Epoch 226/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 226, Loss: 0.04208073392510414\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0293\n",
            "Epoch 227/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 227, Loss: 0.03146009147167206\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0318\n",
            "Epoch 228/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0333Epoch 228, Loss: 0.040200069546699524\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0334\n",
            "Epoch 229/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 229, Loss: 0.00794215314090252\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0276\n",
            "Epoch 230/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0312Epoch 230, Loss: 0.0295009333640337\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0312\n",
            "Epoch 231/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0339Epoch 231, Loss: 0.01568763516843319\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0337\n",
            "Epoch 232/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 232, Loss: 0.02313082106411457\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0301\n",
            "Epoch 233/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 233, Loss: 0.027692750096321106\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0308\n",
            "Epoch 234/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0333Epoch 234, Loss: 0.06443480402231216\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0338\n",
            "Epoch 235/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0342Epoch 235, Loss: 0.02777903899550438\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0341\n",
            "Epoch 236/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 236, Loss: 0.012807060964405537\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0269\n",
            "Epoch 237/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0249Epoch 237, Loss: 0.010937759652733803\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0247\n",
            "Epoch 238/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 238, Loss: 0.030006472021341324\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0290\n",
            "Epoch 239/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0283Epoch 239, Loss: 0.02102610468864441\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0281\n",
            "Epoch 240/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 240, Loss: 0.03511040657758713\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0326\n",
            "Epoch 241/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0325Epoch 241, Loss: 0.04784723371267319\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0327\n",
            "Epoch 242/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0276Epoch 242, Loss: 0.026432150974869728\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0275\n",
            "Epoch 243/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 243, Loss: 0.032810330390930176\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0289\n",
            "Epoch 244/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 244, Loss: 0.031222863122820854\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0303\n",
            "Epoch 245/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 245, Loss: 0.01785583421587944\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0288\n",
            "Epoch 246/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 246, Loss: 0.012080453336238861\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0289\n",
            "Epoch 247/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 247, Loss: 0.012885545380413532\n",
            "63/63 [==============================] - 24s 368ms/step - loss: 0.0299\n",
            "Epoch 248/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 248, Loss: 0.01470278762280941\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0290\n",
            "Epoch 249/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0350Epoch 249, Loss: 0.0524214468896389\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0353\n",
            "Epoch 250/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 250, Loss: 0.021227896213531494\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0306\n",
            "Epoch 251/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 251, Loss: 0.036441318690776825\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0297\n",
            "Epoch 252/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 252, Loss: 0.018962064757943153\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0309\n",
            "Epoch 253/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0255Epoch 253, Loss: 0.02004420757293701\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0254\n",
            "Epoch 254/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 254, Loss: 0.054827459156513214\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0297\n",
            "Epoch 255/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0307Epoch 255, Loss: 0.04749502241611481\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0310\n",
            "Epoch 256/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 256, Loss: 0.02270004153251648\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0273\n",
            "Epoch 257/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 257, Loss: 0.03285803273320198\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0288\n",
            "Epoch 258/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 258, Loss: 0.046268217265605927\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0303\n",
            "Epoch 259/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 259, Loss: 0.014600412920117378\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0278\n",
            "Epoch 260/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 260, Loss: 0.05451178550720215\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0292\n",
            "Epoch 261/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 261, Loss: 0.0503096804022789\n",
            "63/63 [==============================] - 25s 387ms/step - loss: 0.0318\n",
            "Epoch 262/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 262, Loss: 0.03595646098256111\n",
            "63/63 [==============================] - 34s 525ms/step - loss: 0.0264\n",
            "Epoch 263/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 263, Loss: 0.038494307547807693\n",
            "63/63 [==============================] - 39s 602ms/step - loss: 0.0317\n",
            "Epoch 264/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 264, Loss: 0.019459880888462067\n",
            "63/63 [==============================] - 37s 563ms/step - loss: 0.0296\n",
            "Epoch 265/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 265, Loss: 0.015906287357211113\n",
            "63/63 [==============================] - 36s 555ms/step - loss: 0.0293\n",
            "Epoch 266/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 266, Loss: 0.008529397659003735\n",
            "63/63 [==============================] - 41s 632ms/step - loss: 0.0282\n",
            "Epoch 267/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0260Epoch 267, Loss: 0.021512575447559357\n",
            "63/63 [==============================] - 39s 599ms/step - loss: 0.0260\n",
            "Epoch 268/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0287Epoch 268, Loss: 0.004329381510615349\n",
            "63/63 [==============================] - 38s 577ms/step - loss: 0.0284\n",
            "Epoch 269/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 269, Loss: 0.03394623100757599\n",
            "63/63 [==============================] - 41s 627ms/step - loss: 0.0274\n",
            "Epoch 270/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 270, Loss: 0.01973852887749672\n",
            "63/63 [==============================] - 35s 530ms/step - loss: 0.0292\n",
            "Epoch 271/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 271, Loss: 0.06919600069522858\n",
            "63/63 [==============================] - 31s 463ms/step - loss: 0.0297\n",
            "Epoch 272/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 272, Loss: 0.08558003604412079\n",
            "63/63 [==============================] - 27s 418ms/step - loss: 0.0304\n",
            "Epoch 273/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 273, Loss: 0.008499747142195702\n",
            "63/63 [==============================] - 32s 490ms/step - loss: 0.0316\n",
            "Epoch 274/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 274, Loss: 0.016244100406765938\n",
            "63/63 [==============================] - 30s 448ms/step - loss: 0.0295\n",
            "Epoch 275/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 275, Loss: 0.02003341168165207\n",
            "63/63 [==============================] - 31s 470ms/step - loss: 0.0303\n",
            "Epoch 276/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 276, Loss: 0.0426197350025177\n",
            "63/63 [==============================] - 33s 500ms/step - loss: 0.0319\n",
            "Epoch 277/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 277, Loss: 0.019218115136027336\n",
            "63/63 [==============================] - 31s 475ms/step - loss: 0.0288\n",
            "Epoch 278/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 278, Loss: 0.020771468058228493\n",
            "63/63 [==============================] - 29s 439ms/step - loss: 0.0315\n",
            "Epoch 279/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 279, Loss: 0.028254792094230652\n",
            "63/63 [==============================] - 29s 445ms/step - loss: 0.0307\n",
            "Epoch 280/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 280, Loss: 0.027859995141625404\n",
            "63/63 [==============================] - 30s 453ms/step - loss: 0.0292\n",
            "Epoch 281/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 281, Loss: 0.016294483095407486\n",
            "63/63 [==============================] - 30s 454ms/step - loss: 0.0299\n",
            "Epoch 282/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 282, Loss: 0.01662733405828476\n",
            "63/63 [==============================] - 30s 450ms/step - loss: 0.0279\n",
            "Epoch 283/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 283, Loss: 0.038995638489723206\n",
            "63/63 [==============================] - 28s 422ms/step - loss: 0.0304\n",
            "Epoch 284/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 284, Loss: 0.020655719563364983\n",
            "63/63 [==============================] - 28s 417ms/step - loss: 0.0294\n",
            "Epoch 285/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 285, Loss: 0.02798638865351677\n",
            "63/63 [==============================] - 27s 413ms/step - loss: 0.0318\n",
            "Epoch 286/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 286, Loss: 0.018202582374215126\n",
            "63/63 [==============================] - 28s 414ms/step - loss: 0.0292\n",
            "Epoch 287/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 287, Loss: 0.01986514963209629\n",
            "63/63 [==============================] - 27s 409ms/step - loss: 0.0279\n",
            "Epoch 288/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 288, Loss: 0.011509358882904053\n",
            "63/63 [==============================] - 27s 412ms/step - loss: 0.0276\n",
            "Epoch 289/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 289, Loss: 0.04349987581372261\n",
            "63/63 [==============================] - 27s 412ms/step - loss: 0.0319\n",
            "Epoch 290/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 290, Loss: 0.0192765723913908\n",
            "63/63 [==============================] - 27s 415ms/step - loss: 0.0303\n",
            "Epoch 291/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0333Epoch 291, Loss: 0.05941564962267876\n",
            "63/63 [==============================] - 27s 404ms/step - loss: 0.0337\n",
            "Epoch 292/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 292, Loss: 0.03020782768726349\n",
            "63/63 [==============================] - 27s 405ms/step - loss: 0.0297\n",
            "Epoch 293/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 293, Loss: 0.01751876249909401\n",
            "63/63 [==============================] - 28s 430ms/step - loss: 0.0262\n",
            "Epoch 294/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 294, Loss: 0.022353090345859528\n",
            "63/63 [==============================] - 28s 430ms/step - loss: 0.0274\n",
            "Epoch 295/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0333Epoch 295, Loss: 0.049429506063461304\n",
            "63/63 [==============================] - 29s 427ms/step - loss: 0.0335\n",
            "Epoch 296/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 296, Loss: 0.012093565426766872\n",
            "63/63 [==============================] - 29s 440ms/step - loss: 0.0305\n",
            "Epoch 297/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 297, Loss: 0.017643067985773087\n",
            "63/63 [==============================] - 31s 466ms/step - loss: 0.0299\n",
            "Epoch 298/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 298, Loss: 0.04068561643362045\n",
            "63/63 [==============================] - 35s 540ms/step - loss: 0.0292\n",
            "Epoch 299/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 299, Loss: 0.02764633297920227\n",
            "63/63 [==============================] - 34s 517ms/step - loss: 0.0306\n",
            "Epoch 300/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 300, Loss: 0.03035946749150753\n",
            "63/63 [==============================] - 29s 434ms/step - loss: 0.0288\n",
            "Epoch 301/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 301, Loss: 0.0683126300573349\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0306\n",
            "Epoch 302/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 302, Loss: 0.028223130851984024\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0302\n",
            "Epoch 303/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 303, Loss: 0.021846221759915352\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0297\n",
            "Epoch 304/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 304, Loss: 0.012492953799664974\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0268\n",
            "Epoch 305/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 305, Loss: 0.01147716585546732\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0269\n",
            "Epoch 306/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0254Epoch 306, Loss: 0.04533461853861809\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0257\n",
            "Epoch 307/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 307, Loss: 0.02981373481452465\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0299\n",
            "Epoch 308/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 308, Loss: 0.00934644602239132\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0264\n",
            "Epoch 309/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0312Epoch 309, Loss: 0.056237343698740005\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0316\n",
            "Epoch 310/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 310, Loss: 0.021432440727949142\n",
            "63/63 [==============================] - 27s 423ms/step - loss: 0.0299\n",
            "Epoch 311/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 311, Loss: 0.04062190651893616\n",
            "63/63 [==============================] - 30s 458ms/step - loss: 0.0288\n",
            "Epoch 312/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0349Epoch 312, Loss: 0.02877179905772209\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0348\n",
            "Epoch 313/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 313, Loss: 0.009670340456068516\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0277\n",
            "Epoch 314/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 314, Loss: 0.035739898681640625\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0293\n",
            "Epoch 315/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 315, Loss: 0.02040635421872139\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0274\n",
            "Epoch 316/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0334Epoch 316, Loss: 0.006638310384005308\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0330\n",
            "Epoch 317/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 317, Loss: 0.02325940504670143\n",
            "63/63 [==============================] - 25s 390ms/step - loss: 0.0307\n",
            "Epoch 318/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0278Epoch 318, Loss: 0.02425573393702507\n",
            "63/63 [==============================] - 30s 446ms/step - loss: 0.0277\n",
            "Epoch 319/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 319, Loss: 0.04112089052796364\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0315\n",
            "Epoch 320/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 320, Loss: 0.0329144150018692\n",
            "63/63 [==============================] - 27s 430ms/step - loss: 0.0294\n",
            "Epoch 321/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 321, Loss: 0.02775801531970501\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0296\n",
            "Epoch 322/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 322, Loss: 0.03356374800205231\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0278\n",
            "Epoch 323/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 323, Loss: 0.009072359651327133\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0285\n",
            "Epoch 324/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 324, Loss: 0.013186341151595116\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0267\n",
            "Epoch 325/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 325, Loss: 0.01972048357129097\n",
            "63/63 [==============================] - 26s 411ms/step - loss: 0.0312\n",
            "Epoch 326/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 326, Loss: 0.02107146382331848\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0291\n",
            "Epoch 327/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 327, Loss: 0.04713700711727142\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0274\n",
            "Epoch 328/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 328, Loss: 0.013798138126730919\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0282\n",
            "Epoch 329/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0340Epoch 329, Loss: 0.0345228873193264\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0341\n",
            "Epoch 330/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 330, Loss: 0.016218628734350204\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0287\n",
            "Epoch 331/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 331, Loss: 0.017925601452589035\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0270\n",
            "Epoch 332/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 332, Loss: 0.025408819317817688\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0296\n",
            "Epoch 333/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0304Epoch 333, Loss: 0.011912943795323372\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0301\n",
            "Epoch 334/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0312Epoch 334, Loss: 0.012434490956366062\n",
            "63/63 [==============================] - 26s 408ms/step - loss: 0.0309\n",
            "Epoch 335/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 335, Loss: 0.019598206505179405\n",
            "63/63 [==============================] - 29s 433ms/step - loss: 0.0314\n",
            "Epoch 336/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 336, Loss: 0.016707956790924072\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0261\n",
            "Epoch 337/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 337, Loss: 0.03471795469522476\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0297\n",
            "Epoch 338/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 338, Loss: 0.021211301907896996\n",
            "63/63 [==============================] - 26s 413ms/step - loss: 0.0298\n",
            "Epoch 339/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 339, Loss: 0.01961742900311947\n",
            "63/63 [==============================] - 25s 374ms/step - loss: 0.0267\n",
            "Epoch 340/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 340, Loss: 0.03520151972770691\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0287\n",
            "Epoch 341/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 341, Loss: 0.011963258497416973\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0311\n",
            "Epoch 342/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 342, Loss: 0.032070938497781754\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0296\n",
            "Epoch 343/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 343, Loss: 0.018830029293894768\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0275\n",
            "Epoch 344/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0283Epoch 344, Loss: 0.008575230836868286\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0280\n",
            "Epoch 345/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 345, Loss: 0.03962188959121704\n",
            "63/63 [==============================] - 25s 393ms/step - loss: 0.0297\n",
            "Epoch 346/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 346, Loss: 0.0617850124835968\n",
            "63/63 [==============================] - 30s 451ms/step - loss: 0.0313\n",
            "Epoch 347/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0331Epoch 347, Loss: 0.019064510241150856\n",
            "63/63 [==============================] - 30s 453ms/step - loss: 0.0328\n",
            "Epoch 348/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 348, Loss: 0.05585230141878128\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0274\n",
            "Epoch 349/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 349, Loss: 0.026689833030104637\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0264\n",
            "Epoch 350/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 350, Loss: 0.024227704852819443\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0305\n",
            "Epoch 351/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 351, Loss: 0.020584551617503166\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0284\n",
            "Epoch 352/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 352, Loss: 0.021529018878936768\n",
            "63/63 [==============================] - 32s 498ms/step - loss: 0.0289\n",
            "Epoch 353/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 353, Loss: 0.03020082227885723\n",
            "63/63 [==============================] - 31s 470ms/step - loss: 0.0283\n",
            "Epoch 354/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 354, Loss: 0.01654011383652687\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0279\n",
            "Epoch 355/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 355, Loss: 0.030747901648283005\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0308\n",
            "Epoch 356/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 356, Loss: 0.007180436048656702\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0290\n",
            "Epoch 357/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 357, Loss: 0.012371744960546494\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0299\n",
            "Epoch 358/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0252Epoch 358, Loss: 0.0392192080616951\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0254\n",
            "Epoch 359/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 359, Loss: 0.03482499346137047\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0307\n",
            "Epoch 360/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 360, Loss: 0.02007880061864853\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0288\n",
            "Epoch 361/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 361, Loss: 0.01124332845211029\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0267\n",
            "Epoch 362/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 362, Loss: 0.02517414465546608\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0299\n",
            "Epoch 363/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 363, Loss: 0.012159690260887146\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0298\n",
            "Epoch 364/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0334Epoch 364, Loss: 0.06807880103588104\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0339\n",
            "Epoch 365/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 365, Loss: 0.0336628220975399\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0317\n",
            "Epoch 366/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 366, Loss: 0.030579084530472755\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0267\n",
            "Epoch 367/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 367, Loss: 0.013778170570731163\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0283\n",
            "Epoch 368/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 368, Loss: 0.01670878753066063\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0280\n",
            "Epoch 369/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0330Epoch 369, Loss: 0.010592622682452202\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0327\n",
            "Epoch 370/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 370, Loss: 0.004908760078251362\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0282\n",
            "Epoch 371/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 371, Loss: 0.014995201490819454\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0286\n",
            "Epoch 372/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 372, Loss: 0.01680375635623932\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0294\n",
            "Epoch 373/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0246Epoch 373, Loss: 0.013108517043292522\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0244\n",
            "Epoch 374/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 374, Loss: 0.055121518671512604\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0302\n",
            "Epoch 375/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 375, Loss: 0.028432857245206833\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0262\n",
            "Epoch 376/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 376, Loss: 0.017480188980698586\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0288\n",
            "Epoch 377/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 377, Loss: 0.020889248698949814\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0279\n",
            "Epoch 378/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 378, Loss: 0.04718104377388954\n",
            "63/63 [==============================] - 32s 498ms/step - loss: 0.0305\n",
            "Epoch 379/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 379, Loss: 0.02510826662182808\n",
            "63/63 [==============================] - 37s 577ms/step - loss: 0.0304\n",
            "Epoch 380/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0253Epoch 380, Loss: 0.07901789993047714\n",
            "63/63 [==============================] - 37s 553ms/step - loss: 0.0262\n",
            "Epoch 381/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 381, Loss: 0.011369955725967884\n",
            "63/63 [==============================] - 36s 544ms/step - loss: 0.0282\n",
            "Epoch 382/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 382, Loss: 0.010834185406565666\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0292\n",
            "Epoch 383/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 383, Loss: 0.015507703647017479\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0272\n",
            "Epoch 384/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 384, Loss: 0.04255332052707672\n",
            "63/63 [==============================] - 30s 479ms/step - loss: 0.0320\n",
            "Epoch 385/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 385, Loss: 0.01664966717362404\n",
            "63/63 [==============================] - 27s 404ms/step - loss: 0.0326\n",
            "Epoch 386/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 386, Loss: 0.010411127470433712\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0270\n",
            "Epoch 387/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 387, Loss: 0.021488886326551437\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0301\n",
            "Epoch 388/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 388, Loss: 0.027622176334261894\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0275\n",
            "Epoch 389/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 389, Loss: 0.024952422827482224\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0271\n",
            "Epoch 390/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 390, Loss: 0.03106946125626564\n",
            "63/63 [==============================] - 27s 433ms/step - loss: 0.0315\n",
            "Epoch 391/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 391, Loss: 0.018228568136692047\n",
            "63/63 [==============================] - 35s 528ms/step - loss: 0.0300\n",
            "Epoch 392/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 392, Loss: 0.020493902266025543\n",
            "63/63 [==============================] - 34s 513ms/step - loss: 0.0288\n",
            "Epoch 393/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0287Epoch 393, Loss: 0.0215267576277256\n",
            "63/63 [==============================] - 34s 522ms/step - loss: 0.0286\n",
            "Epoch 394/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 394, Loss: 0.021949071437120438\n",
            "63/63 [==============================] - 32s 489ms/step - loss: 0.0285\n",
            "Epoch 395/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0249Epoch 395, Loss: 0.038763727992773056\n",
            "63/63 [==============================] - 25s 370ms/step - loss: 0.0252\n",
            "Epoch 396/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 396, Loss: 0.005753754172474146\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0264\n",
            "Epoch 397/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0283Epoch 397, Loss: 0.016560494899749756\n",
            "63/63 [==============================] - 25s 391ms/step - loss: 0.0281\n",
            "Epoch 398/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0249Epoch 398, Loss: 0.013604415580630302\n",
            "63/63 [==============================] - 31s 468ms/step - loss: 0.0248\n",
            "Epoch 399/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 399, Loss: 0.020282082259655\n",
            "63/63 [==============================] - 32s 487ms/step - loss: 0.0289\n",
            "Epoch 400/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 400, Loss: 0.05876536667346954\n",
            "63/63 [==============================] - 33s 496ms/step - loss: 0.0278\n",
            "Epoch 401/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 401, Loss: 0.015503767877817154\n",
            "63/63 [==============================] - 31s 461ms/step - loss: 0.0284\n",
            "Epoch 402/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 402, Loss: 0.03206580504775047\n",
            "63/63 [==============================] - 33s 492ms/step - loss: 0.0294\n",
            "Epoch 403/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 403, Loss: 0.011978043243288994\n",
            "63/63 [==============================] - 32s 482ms/step - loss: 0.0289\n",
            "Epoch 404/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 404, Loss: 0.024427413940429688\n",
            "63/63 [==============================] - 32s 495ms/step - loss: 0.0307\n",
            "Epoch 405/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0315Epoch 405, Loss: 0.020896852016448975\n",
            "63/63 [==============================] - 31s 466ms/step - loss: 0.0313\n",
            "Epoch 406/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 406, Loss: 0.022296063601970673\n",
            "63/63 [==============================] - 32s 483ms/step - loss: 0.0327\n",
            "Epoch 407/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0278Epoch 407, Loss: 0.009735855273902416\n",
            "63/63 [==============================] - 32s 492ms/step - loss: 0.0276\n",
            "Epoch 408/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 408, Loss: 0.01514054462313652\n",
            "63/63 [==============================] - 32s 492ms/step - loss: 0.0283\n",
            "Epoch 409/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 409, Loss: 0.01325942762196064\n",
            "63/63 [==============================] - 25s 370ms/step - loss: 0.0295\n",
            "Epoch 410/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 410, Loss: 0.025161322206258774\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0292\n",
            "Epoch 411/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0345Epoch 411, Loss: 0.011415746062994003\n",
            "63/63 [==============================] - 24s 368ms/step - loss: 0.0341\n",
            "Epoch 412/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0276Epoch 412, Loss: 0.026326557621359825\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0276\n",
            "Epoch 413/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 413, Loss: 0.03276609629392624\n",
            "63/63 [==============================] - 24s 368ms/step - loss: 0.0318\n",
            "Epoch 414/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0251Epoch 414, Loss: 0.03677957504987717\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0253\n",
            "Epoch 415/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0283Epoch 415, Loss: 0.02266399748623371\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0282\n",
            "Epoch 416/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 416, Loss: 0.025221360847353935\n",
            "63/63 [==============================] - 26s 417ms/step - loss: 0.0297\n",
            "Epoch 417/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 417, Loss: 0.034613609313964844\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0295\n",
            "Epoch 418/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 418, Loss: 0.04596276581287384\n",
            "63/63 [==============================] - 28s 441ms/step - loss: 0.0299\n",
            "Epoch 419/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0322Epoch 419, Loss: 0.01916586607694626\n",
            "63/63 [==============================] - 25s 392ms/step - loss: 0.0320\n",
            "Epoch 420/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 420, Loss: 0.03338034823536873\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0280\n",
            "Epoch 421/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0287Epoch 421, Loss: 0.0338236540555954\n",
            "63/63 [==============================] - 36s 541ms/step - loss: 0.0288\n",
            "Epoch 422/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 422, Loss: 0.020840201526880264\n",
            "63/63 [==============================] - 34s 526ms/step - loss: 0.0292\n",
            "Epoch 423/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0251Epoch 423, Loss: 0.006209569051861763\n",
            "63/63 [==============================] - 36s 551ms/step - loss: 0.0248\n",
            "Epoch 424/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0322Epoch 424, Loss: 0.015700995922088623\n",
            "63/63 [==============================] - 40s 611ms/step - loss: 0.0319\n",
            "Epoch 425/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 425, Loss: 0.026926063001155853\n",
            "63/63 [==============================] - 38s 574ms/step - loss: 0.0326\n",
            "Epoch 426/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 426, Loss: 0.028786074370145798\n",
            "63/63 [==============================] - 35s 530ms/step - loss: 0.0262\n",
            "Epoch 427/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 427, Loss: 0.037586573511362076\n",
            "63/63 [==============================] - 39s 590ms/step - loss: 0.0298\n",
            "Epoch 428/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 428, Loss: 0.02477790229022503\n",
            "63/63 [==============================] - 40s 607ms/step - loss: 0.0292\n",
            "Epoch 429/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 429, Loss: 0.05990196019411087\n",
            "63/63 [==============================] - 36s 550ms/step - loss: 0.0294\n",
            "Epoch 430/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 430, Loss: 0.058744803071022034\n",
            "63/63 [==============================] - 39s 596ms/step - loss: 0.0267\n",
            "Epoch 431/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 431, Loss: 0.0465673953294754\n",
            "63/63 [==============================] - 37s 563ms/step - loss: 0.0270\n",
            "Epoch 432/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 432, Loss: 0.006737328600138426\n",
            "63/63 [==============================] - 34s 526ms/step - loss: 0.0261\n",
            "Epoch 433/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 433, Loss: 0.07214298844337463\n",
            "63/63 [==============================] - 33s 501ms/step - loss: 0.0293\n",
            "Epoch 434/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 434, Loss: 0.031664133071899414\n",
            "63/63 [==============================] - 31s 465ms/step - loss: 0.0298\n",
            "Epoch 435/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 435, Loss: 0.02162853255867958\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0284\n",
            "Epoch 436/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0287Epoch 436, Loss: 0.019468503072857857\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0285\n",
            "Epoch 437/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 437, Loss: 0.03019310161471367\n",
            "63/63 [==============================] - 25s 400ms/step - loss: 0.0289\n",
            "Epoch 438/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 438, Loss: 0.03775915130972862\n",
            "63/63 [==============================] - 26s 405ms/step - loss: 0.0285\n",
            "Epoch 439/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 439, Loss: 0.026587963104248047\n",
            "63/63 [==============================] - 31s 472ms/step - loss: 0.0285\n",
            "Epoch 440/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 440, Loss: 0.03894103690981865\n",
            "63/63 [==============================] - 31s 474ms/step - loss: 0.0309\n",
            "Epoch 441/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0332Epoch 441, Loss: 0.023764587938785553\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0331\n",
            "Epoch 442/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0236Epoch 442, Loss: 0.02307705581188202\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0236\n",
            "Epoch 443/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 443, Loss: 0.05537742003798485\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0312\n",
            "Epoch 444/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 444, Loss: 0.012334071099758148\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0277\n",
            "Epoch 445/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 445, Loss: 0.018839629366993904\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0266\n",
            "Epoch 446/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 446, Loss: 0.014835643582046032\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0295\n",
            "Epoch 447/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 447, Loss: 0.04594992846250534\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0308\n",
            "Epoch 448/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0276Epoch 448, Loss: 0.013397933915257454\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0273\n",
            "Epoch 449/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 449, Loss: 0.03428059443831444\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0294\n",
            "Epoch 450/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 450, Loss: 0.03246525675058365\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0314\n",
            "Epoch 451/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 451, Loss: 0.010549266822636127\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0266\n",
            "Epoch 452/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 452, Loss: 0.026438558474183083\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0294\n",
            "Epoch 453/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 453, Loss: 0.033239997923374176\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0263\n",
            "Epoch 454/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 454, Loss: 0.005402741022408009\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0285\n",
            "Epoch 455/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 455, Loss: 0.05068957060575485\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0317\n",
            "Epoch 456/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 456, Loss: 0.00988251343369484\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0276\n",
            "Epoch 457/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 457, Loss: 0.03377098590135574\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0290\n",
            "Epoch 458/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 458, Loss: 0.04164895415306091\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0331\n",
            "Epoch 459/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 459, Loss: 0.04735513776540756\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0304\n",
            "Epoch 460/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0256Epoch 460, Loss: 0.046615730971097946\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0259\n",
            "Epoch 461/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0261Epoch 461, Loss: 0.02941092848777771\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0261\n",
            "Epoch 462/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0321Epoch 462, Loss: 0.039631977677345276\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0323\n",
            "Epoch 463/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 463, Loss: 0.020980708301067352\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0302\n",
            "Epoch 464/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 464, Loss: 0.02990027330815792\n",
            "63/63 [==============================] - 31s 490ms/step - loss: 0.0299\n",
            "Epoch 465/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 465, Loss: 0.027661513537168503\n",
            "63/63 [==============================] - 35s 540ms/step - loss: 0.0282\n",
            "Epoch 466/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 466, Loss: 0.018429718911647797\n",
            "63/63 [==============================] - 40s 609ms/step - loss: 0.0272\n",
            "Epoch 467/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 467, Loss: 0.022885538637638092\n",
            "63/63 [==============================] - 36s 545ms/step - loss: 0.0294\n",
            "Epoch 468/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 468, Loss: 0.016358505934476852\n",
            "63/63 [==============================] - 36s 540ms/step - loss: 0.0288\n",
            "Epoch 469/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 469, Loss: 0.024493131786584854\n",
            "63/63 [==============================] - 37s 557ms/step - loss: 0.0300\n",
            "Epoch 470/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 470, Loss: 0.04337075352668762\n",
            "63/63 [==============================] - 37s 560ms/step - loss: 0.0274\n",
            "Epoch 471/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 471, Loss: 0.04614783823490143\n",
            "63/63 [==============================] - 37s 565ms/step - loss: 0.0300\n",
            "Epoch 472/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0259Epoch 472, Loss: 0.03677918761968613\n",
            "63/63 [==============================] - 31s 460ms/step - loss: 0.0261\n",
            "Epoch 473/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 473, Loss: 0.025645600631833076\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0293\n",
            "Epoch 474/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 474, Loss: 0.044167280197143555\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0309\n",
            "Epoch 475/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 475, Loss: 0.03816617652773857\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0317\n",
            "Epoch 476/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 476, Loss: 0.013469818979501724\n",
            "63/63 [==============================] - 25s 396ms/step - loss: 0.0271\n",
            "Epoch 477/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 477, Loss: 0.0648869127035141\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0299\n",
            "Epoch 478/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 478, Loss: 0.017499316483736038\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0275\n",
            "Epoch 479/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 479, Loss: 0.007519985549151897\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0274\n",
            "Epoch 480/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 480, Loss: 0.023148877546191216\n",
            "63/63 [==============================] - 30s 453ms/step - loss: 0.0299\n",
            "Epoch 481/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0327Epoch 481, Loss: 0.01318074855953455\n",
            "63/63 [==============================] - 26s 380ms/step - loss: 0.0324\n",
            "Epoch 482/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 482, Loss: 0.026606418192386627\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0302\n",
            "Epoch 483/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 483, Loss: 0.020452560856938362\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0311\n",
            "Epoch 484/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 484, Loss: 0.029314236715435982\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0275\n",
            "Epoch 485/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0253Epoch 485, Loss: 0.02160484530031681\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0253\n",
            "Epoch 486/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 486, Loss: 0.032736558467149734\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0286\n",
            "Epoch 487/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 487, Loss: 0.04237910732626915\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0291\n",
            "Epoch 488/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0261Epoch 488, Loss: 0.01740095019340515\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0259\n",
            "Epoch 489/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0363Epoch 489, Loss: 0.020233191549777985\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0360\n",
            "Epoch 490/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 490, Loss: 0.06601271778345108\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0316\n",
            "Epoch 491/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0257Epoch 491, Loss: 0.005301715340465307\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0253\n",
            "Epoch 492/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 492, Loss: 0.06415116041898727\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0304\n",
            "Epoch 493/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 493, Loss: 0.020275719463825226\n",
            "63/63 [==============================] - 27s 428ms/step - loss: 0.0305\n",
            "Epoch 494/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 494, Loss: 0.007638343144208193\n",
            "63/63 [==============================] - 24s 367ms/step - loss: 0.0276\n",
            "Epoch 495/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 495, Loss: 0.09401491284370422\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0324\n",
            "Epoch 496/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0266Epoch 496, Loss: 0.02173713967204094\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0265\n",
            "Epoch 497/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 497, Loss: 0.00695947278290987\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0297\n",
            "Epoch 498/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 498, Loss: 0.07456225901842117\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0287\n",
            "Epoch 499/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 499, Loss: 0.009171574376523495\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0292\n",
            "Epoch 500/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 500, Loss: 0.04170616716146469\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0277\n",
            "Epoch 501/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0307Epoch 501, Loss: 0.009773625060915947\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0304\n",
            "Epoch 502/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 502, Loss: 0.037559524178504944\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0326\n",
            "Epoch 503/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 503, Loss: 0.008994519710540771\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0276\n",
            "Epoch 504/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0247Epoch 504, Loss: 0.041070543229579926\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0250\n",
            "Epoch 505/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 505, Loss: 0.03329465165734291\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0280\n",
            "Epoch 506/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0309Epoch 506, Loss: 0.0286317877471447\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0308\n",
            "Epoch 507/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 507, Loss: 0.03079242631793022\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0268\n",
            "Epoch 508/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 508, Loss: 0.02718878909945488\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0293\n",
            "Epoch 509/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0318Epoch 509, Loss: 0.030926596373319626\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0318\n",
            "Epoch 510/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 510, Loss: 0.04093996062874794\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0272\n",
            "Epoch 511/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 511, Loss: 0.03432246297597885\n",
            "63/63 [==============================] - 25s 386ms/step - loss: 0.0313\n",
            "Epoch 512/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 512, Loss: 0.03381400927901268\n",
            "63/63 [==============================] - 25s 384ms/step - loss: 0.0278\n",
            "Epoch 513/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0326Epoch 513, Loss: 0.04827960953116417\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0328\n",
            "Epoch 514/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 514, Loss: 0.032705970108509064\n",
            "63/63 [==============================] - 25s 387ms/step - loss: 0.0299\n",
            "Epoch 515/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 515, Loss: 0.05448096618056297\n",
            "63/63 [==============================] - 25s 385ms/step - loss: 0.0305\n",
            "Epoch 516/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0266Epoch 516, Loss: 0.03535831719636917\n",
            "63/63 [==============================] - 24s 383ms/step - loss: 0.0267\n",
            "Epoch 517/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 517, Loss: 0.023532608523964882\n",
            "63/63 [==============================] - 24s 382ms/step - loss: 0.0298\n",
            "Epoch 518/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 518, Loss: 0.030642542988061905\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0298\n",
            "Epoch 519/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 519, Loss: 0.01478642225265503\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0265\n",
            "Epoch 520/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 520, Loss: 0.040278416126966476\n",
            "63/63 [==============================] - 24s 381ms/step - loss: 0.0286\n",
            "Epoch 521/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 521, Loss: 0.02842818573117256\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0270\n",
            "Epoch 522/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 522, Loss: 0.011040699668228626\n",
            "63/63 [==============================] - 24s 378ms/step - loss: 0.0268\n",
            "Epoch 523/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 523, Loss: 0.020640432834625244\n",
            "63/63 [==============================] - 24s 377ms/step - loss: 0.0284\n",
            "Epoch 524/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 524, Loss: 0.024678124114871025\n",
            "63/63 [==============================] - 24s 380ms/step - loss: 0.0272\n",
            "Epoch 525/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 525, Loss: 0.0055448454804718494\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0261\n",
            "Epoch 526/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 526, Loss: 0.03167366236448288\n",
            "63/63 [==============================] - 24s 379ms/step - loss: 0.0271\n",
            "Epoch 527/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 527, Loss: 0.0438852533698082\n",
            "63/63 [==============================] - 24s 376ms/step - loss: 0.0280\n",
            "Epoch 528/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 528, Loss: 0.039175406098365784\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0300\n",
            "Epoch 529/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 529, Loss: 0.024157315492630005\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0283\n",
            "Epoch 530/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 530, Loss: 0.014800392091274261\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0269\n",
            "Epoch 531/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 531, Loss: 0.03414161503314972\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0270\n",
            "Epoch 532/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 532, Loss: 0.02032453380525112\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0271\n",
            "Epoch 533/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 533, Loss: 0.013653387315571308\n",
            "63/63 [==============================] - 30s 459ms/step - loss: 0.0270\n",
            "Epoch 534/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0304Epoch 534, Loss: 0.017362121492624283\n",
            "63/63 [==============================] - 32s 482ms/step - loss: 0.0302\n",
            "Epoch 535/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0255Epoch 535, Loss: 0.027051709592342377\n",
            "63/63 [==============================] - 33s 502ms/step - loss: 0.0255\n",
            "Epoch 536/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 536, Loss: 0.03662049025297165\n",
            "63/63 [==============================] - 34s 513ms/step - loss: 0.0299\n",
            "Epoch 537/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0332Epoch 537, Loss: 0.05657266080379486\n",
            "63/63 [==============================] - 33s 497ms/step - loss: 0.0336\n",
            "Epoch 538/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 538, Loss: 0.012504005804657936\n",
            "63/63 [==============================] - 34s 517ms/step - loss: 0.0295\n",
            "Epoch 539/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 539, Loss: 0.02915939874947071\n",
            "63/63 [==============================] - 34s 524ms/step - loss: 0.0290\n",
            "Epoch 540/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 540, Loss: 0.029720420017838478\n",
            "63/63 [==============================] - 35s 528ms/step - loss: 0.0282\n",
            "Epoch 541/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 541, Loss: 0.03294552117586136\n",
            "63/63 [==============================] - 34s 515ms/step - loss: 0.0294\n",
            "Epoch 542/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 542, Loss: 0.03668760880827904\n",
            "63/63 [==============================] - 34s 509ms/step - loss: 0.0292\n",
            "Epoch 543/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 543, Loss: 0.015133213251829147\n",
            "63/63 [==============================] - 35s 534ms/step - loss: 0.0261\n",
            "Epoch 544/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0303Epoch 544, Loss: 0.03930409997701645\n",
            "63/63 [==============================] - 34s 510ms/step - loss: 0.0305\n",
            "Epoch 545/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 545, Loss: 0.02273131161928177\n",
            "63/63 [==============================] - 32s 485ms/step - loss: 0.0297\n",
            "Epoch 546/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 546, Loss: 0.025371259078383446\n",
            "63/63 [==============================] - 29s 428ms/step - loss: 0.0295\n",
            "Epoch 547/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 547, Loss: 0.021762719377875328\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0278\n",
            "Epoch 548/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 548, Loss: 0.010920513421297073\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0274\n",
            "Epoch 549/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 549, Loss: 0.01707274466753006\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0277\n",
            "Epoch 550/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0250Epoch 550, Loss: 0.020458728075027466\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0249\n",
            "Epoch 551/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 551, Loss: 0.010080436244606972\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0265\n",
            "Epoch 552/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 552, Loss: 0.020425625145435333\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0266\n",
            "Epoch 553/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 553, Loss: 0.020294390618801117\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0303\n",
            "Epoch 554/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 554, Loss: 0.005672774277627468\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0294\n",
            "Epoch 555/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0278Epoch 555, Loss: 0.03817132115364075\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0279\n",
            "Epoch 556/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 556, Loss: 0.03432755917310715\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0295\n",
            "Epoch 557/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 557, Loss: 0.06202425807714462\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0305\n",
            "Epoch 558/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 558, Loss: 0.02207034081220627\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0317\n",
            "Epoch 559/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 559, Loss: 0.030975336208939552\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0265\n",
            "Epoch 560/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 560, Loss: 0.038669001311063766\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0313\n",
            "Epoch 561/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 561, Loss: 0.00748109444975853\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0309\n",
            "Epoch 562/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0329Epoch 562, Loss: 0.05633876472711563\n",
            "63/63 [==============================] - 28s 437ms/step - loss: 0.0333\n",
            "Epoch 563/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 563, Loss: 0.023468006402254105\n",
            "63/63 [==============================] - 33s 502ms/step - loss: 0.0299\n",
            "Epoch 564/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0283Epoch 564, Loss: 0.030506115406751633\n",
            "63/63 [==============================] - 32s 482ms/step - loss: 0.0284\n",
            "Epoch 565/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 565, Loss: 0.02168061025440693\n",
            "63/63 [==============================] - 33s 494ms/step - loss: 0.0264\n",
            "Epoch 566/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 566, Loss: 0.05162956565618515\n",
            "63/63 [==============================] - 34s 505ms/step - loss: 0.0273\n",
            "Epoch 567/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 567, Loss: 0.026147684082388878\n",
            "63/63 [==============================] - 36s 550ms/step - loss: 0.0291\n",
            "Epoch 568/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 568, Loss: 0.019296593964099884\n",
            "63/63 [==============================] - 35s 534ms/step - loss: 0.0303\n",
            "Epoch 569/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 569, Loss: 0.011948971077799797\n",
            "63/63 [==============================] - 38s 568ms/step - loss: 0.0292\n",
            "Epoch 570/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 570, Loss: 0.026551052927970886\n",
            "63/63 [==============================] - 33s 509ms/step - loss: 0.0271\n",
            "Epoch 571/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 571, Loss: 0.02774372696876526\n",
            "63/63 [==============================] - 33s 490ms/step - loss: 0.0285\n",
            "Epoch 572/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 572, Loss: 0.018178004771471024\n",
            "63/63 [==============================] - 35s 538ms/step - loss: 0.0283\n",
            "Epoch 573/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0255Epoch 573, Loss: 0.024926045909523964\n",
            "63/63 [==============================] - 33s 501ms/step - loss: 0.0255\n",
            "Epoch 574/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0250Epoch 574, Loss: 0.022253945469856262\n",
            "63/63 [==============================] - 34s 517ms/step - loss: 0.0250\n",
            "Epoch 575/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0261Epoch 575, Loss: 0.02598915807902813\n",
            "63/63 [==============================] - 34s 512ms/step - loss: 0.0261\n",
            "Epoch 576/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 576, Loss: 0.02206205204129219\n",
            "63/63 [==============================] - 34s 510ms/step - loss: 0.0280\n",
            "Epoch 577/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0251Epoch 577, Loss: 0.012487880885601044\n",
            "63/63 [==============================] - 31s 457ms/step - loss: 0.0249\n",
            "Epoch 578/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0283Epoch 578, Loss: 0.025422178208827972\n",
            "63/63 [==============================] - 30s 459ms/step - loss: 0.0282\n",
            "Epoch 579/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0327Epoch 579, Loss: 0.0282154381275177\n",
            "63/63 [==============================] - 30s 459ms/step - loss: 0.0326\n",
            "Epoch 580/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 580, Loss: 0.00833155307918787\n",
            "63/63 [==============================] - 31s 474ms/step - loss: 0.0271\n",
            "Epoch 581/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0322Epoch 581, Loss: 0.014418715611100197\n",
            "63/63 [==============================] - 29s 444ms/step - loss: 0.0319\n",
            "Epoch 582/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 582, Loss: 0.025792473927140236\n",
            "63/63 [==============================] - 31s 477ms/step - loss: 0.0299\n",
            "Epoch 583/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 583, Loss: 0.014442000538110733\n",
            "63/63 [==============================] - 32s 484ms/step - loss: 0.0272\n",
            "Epoch 584/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 584, Loss: 0.012579791247844696\n",
            "63/63 [==============================] - 30s 460ms/step - loss: 0.0273\n",
            "Epoch 585/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 585, Loss: 0.02653023600578308\n",
            "63/63 [==============================] - 33s 496ms/step - loss: 0.0280\n",
            "Epoch 586/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 586, Loss: 0.017137687653303146\n",
            "63/63 [==============================] - 30s 465ms/step - loss: 0.0289\n",
            "Epoch 587/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 587, Loss: 0.04951632767915726\n",
            "63/63 [==============================] - 34s 512ms/step - loss: 0.0285\n",
            "Epoch 588/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 588, Loss: 0.029745519161224365\n",
            "63/63 [==============================] - 33s 511ms/step - loss: 0.0265\n",
            "Epoch 589/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 589, Loss: 0.02041633240878582\n",
            "63/63 [==============================] - 34s 526ms/step - loss: 0.0279\n",
            "Epoch 590/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0320Epoch 590, Loss: 0.02722153440117836\n",
            "63/63 [==============================] - 35s 528ms/step - loss: 0.0319\n",
            "Epoch 591/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0332Epoch 591, Loss: 0.03650937229394913\n",
            "63/63 [==============================] - 37s 567ms/step - loss: 0.0332\n",
            "Epoch 592/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0276Epoch 592, Loss: 0.051441740244627\n",
            "63/63 [==============================] - 26s 388ms/step - loss: 0.0280\n",
            "Epoch 593/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 593, Loss: 0.03368672728538513\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0306\n",
            "Epoch 594/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 594, Loss: 0.0475974977016449\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0300\n",
            "Epoch 595/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 595, Loss: 0.02629614621400833\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0300\n",
            "Epoch 596/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 596, Loss: 0.01886644773185253\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0289\n",
            "Epoch 597/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 597, Loss: 0.03507503867149353\n",
            "63/63 [==============================] - 28s 435ms/step - loss: 0.0293\n",
            "Epoch 598/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 598, Loss: 0.031214430928230286\n",
            "63/63 [==============================] - 35s 533ms/step - loss: 0.0282\n",
            "Epoch 599/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0327Epoch 599, Loss: 0.023627137765288353\n",
            "63/63 [==============================] - 34s 523ms/step - loss: 0.0325\n",
            "Epoch 600/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0287Epoch 600, Loss: 0.022694313898682594\n",
            "63/63 [==============================] - 30s 458ms/step - loss: 0.0286\n",
            "Epoch 601/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 601, Loss: 0.0062043266370892525\n",
            "63/63 [==============================] - 32s 484ms/step - loss: 0.0292\n",
            "Epoch 602/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 602, Loss: 0.032195933163166046\n",
            "63/63 [==============================] - 34s 509ms/step - loss: 0.0266\n",
            "Epoch 603/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 603, Loss: 0.019122648984193802\n",
            "63/63 [==============================] - 36s 548ms/step - loss: 0.0273\n",
            "Epoch 604/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0236Epoch 604, Loss: 0.04713774845004082\n",
            "63/63 [==============================] - 35s 524ms/step - loss: 0.0240\n",
            "Epoch 605/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0257Epoch 605, Loss: 0.03456322103738785\n",
            "63/63 [==============================] - 33s 508ms/step - loss: 0.0258\n",
            "Epoch 606/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 606, Loss: 0.012853365391492844\n",
            "63/63 [==============================] - 37s 565ms/step - loss: 0.0283\n",
            "Epoch 607/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 607, Loss: 0.022104032337665558\n",
            "63/63 [==============================] - 36s 539ms/step - loss: 0.0294\n",
            "Epoch 608/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 608, Loss: 0.03821532428264618\n",
            "63/63 [==============================] - 35s 535ms/step - loss: 0.0274\n",
            "Epoch 609/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 609, Loss: 0.014570320025086403\n",
            "63/63 [==============================] - 38s 576ms/step - loss: 0.0287\n",
            "Epoch 610/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0250Epoch 610, Loss: 0.032907404005527496\n",
            "63/63 [==============================] - 37s 560ms/step - loss: 0.0251\n",
            "Epoch 611/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0311Epoch 611, Loss: 0.005840889178216457\n",
            "63/63 [==============================] - 34s 512ms/step - loss: 0.0307\n",
            "Epoch 612/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 612, Loss: 0.022144675254821777\n",
            "63/63 [==============================] - 36s 544ms/step - loss: 0.0290\n",
            "Epoch 613/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 613, Loss: 0.0241911169141531\n",
            "63/63 [==============================] - 37s 566ms/step - loss: 0.0281\n",
            "Epoch 614/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0310Epoch 614, Loss: 0.03063914366066456\n",
            "63/63 [==============================] - 37s 570ms/step - loss: 0.0310\n",
            "Epoch 615/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 615, Loss: 0.013557709753513336\n",
            "63/63 [==============================] - 35s 532ms/step - loss: 0.0266\n",
            "Epoch 616/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 616, Loss: 0.02438656985759735\n",
            "63/63 [==============================] - 37s 557ms/step - loss: 0.0274\n",
            "Epoch 617/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 617, Loss: 0.02651328220963478\n",
            "63/63 [==============================] - 36s 555ms/step - loss: 0.0282\n",
            "Epoch 618/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 618, Loss: 0.013679681345820427\n",
            "63/63 [==============================] - 36s 545ms/step - loss: 0.0283\n",
            "Epoch 619/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 619, Loss: 0.021165721118450165\n",
            "63/63 [==============================] - 36s 550ms/step - loss: 0.0292\n",
            "Epoch 620/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 620, Loss: 0.024842631071805954\n",
            "63/63 [==============================] - 36s 546ms/step - loss: 0.0269\n",
            "Epoch 621/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0287Epoch 621, Loss: 0.03725989907979965\n",
            "63/63 [==============================] - 36s 544ms/step - loss: 0.0288\n",
            "Epoch 622/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 622, Loss: 0.03646243363618851\n",
            "63/63 [==============================] - 33s 506ms/step - loss: 0.0307\n",
            "Epoch 623/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0260Epoch 623, Loss: 0.0233328677713871\n",
            "63/63 [==============================] - 37s 559ms/step - loss: 0.0259\n",
            "Epoch 624/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0276Epoch 624, Loss: 0.038322027772665024\n",
            "63/63 [==============================] - 35s 539ms/step - loss: 0.0277\n",
            "Epoch 625/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 625, Loss: 0.010218658484518528\n",
            "63/63 [==============================] - 37s 565ms/step - loss: 0.0268\n",
            "Epoch 626/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 626, Loss: 0.03750789165496826\n",
            "63/63 [==============================] - 38s 587ms/step - loss: 0.0297\n",
            "Epoch 627/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 627, Loss: 0.049989230930805206\n",
            "63/63 [==============================] - 38s 574ms/step - loss: 0.0296\n",
            "Epoch 628/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 628, Loss: 0.030007341876626015\n",
            "63/63 [==============================] - 37s 571ms/step - loss: 0.0296\n",
            "Epoch 629/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 629, Loss: 0.009806621819734573\n",
            "63/63 [==============================] - 38s 584ms/step - loss: 0.0278\n",
            "Epoch 630/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 630, Loss: 0.013098305091261864\n",
            "63/63 [==============================] - 36s 545ms/step - loss: 0.0261\n",
            "Epoch 631/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 631, Loss: 0.05031581223011017\n",
            "63/63 [==============================] - 35s 525ms/step - loss: 0.0311\n",
            "Epoch 632/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 632, Loss: 0.030336527153849602\n",
            "63/63 [==============================] - 32s 489ms/step - loss: 0.0275\n",
            "Epoch 633/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0273Epoch 633, Loss: 0.024898424744606018\n",
            "63/63 [==============================] - 33s 495ms/step - loss: 0.0272\n",
            "Epoch 634/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 634, Loss: 0.025621909648180008\n",
            "63/63 [==============================] - 26s 384ms/step - loss: 0.0307\n",
            "Epoch 635/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0304Epoch 635, Loss: 0.01914682239294052\n",
            "63/63 [==============================] - 32s 498ms/step - loss: 0.0303\n",
            "Epoch 636/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 636, Loss: 0.016756571829319\n",
            "63/63 [==============================] - 34s 525ms/step - loss: 0.0273\n",
            "Epoch 637/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0324Epoch 637, Loss: 0.03710484504699707\n",
            "63/63 [==============================] - 31s 468ms/step - loss: 0.0325\n",
            "Epoch 638/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 638, Loss: 0.03266880288720131\n",
            "63/63 [==============================] - 29s 441ms/step - loss: 0.0284\n",
            "Epoch 639/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0246Epoch 639, Loss: 0.020047953352332115\n",
            "63/63 [==============================] - 33s 502ms/step - loss: 0.0246\n",
            "Epoch 640/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 640, Loss: 0.060973867774009705\n",
            "63/63 [==============================] - 30s 459ms/step - loss: 0.0303\n",
            "Epoch 641/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 641, Loss: 0.0233015026897192\n",
            "63/63 [==============================] - 31s 474ms/step - loss: 0.0280\n",
            "Epoch 642/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 642, Loss: 0.005513868294656277\n",
            "63/63 [==============================] - 32s 485ms/step - loss: 0.0269\n",
            "Epoch 643/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 643, Loss: 0.02320755273103714\n",
            "63/63 [==============================] - 34s 522ms/step - loss: 0.0270\n",
            "Epoch 644/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 644, Loss: 0.0454656220972538\n",
            "63/63 [==============================] - 33s 497ms/step - loss: 0.0283\n",
            "Epoch 645/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0256Epoch 645, Loss: 0.041583966463804245\n",
            "63/63 [==============================] - 32s 473ms/step - loss: 0.0258\n",
            "Epoch 646/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 646, Loss: 0.02820308692753315\n",
            "63/63 [==============================] - 33s 494ms/step - loss: 0.0270\n",
            "Epoch 647/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 647, Loss: 0.05710398405790329\n",
            "63/63 [==============================] - 31s 473ms/step - loss: 0.0269\n",
            "Epoch 648/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0251Epoch 648, Loss: 0.01045071892440319\n",
            "63/63 [==============================] - 31s 473ms/step - loss: 0.0249\n",
            "Epoch 649/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0309Epoch 649, Loss: 0.022409239783883095\n",
            "63/63 [==============================] - 30s 461ms/step - loss: 0.0308\n",
            "Epoch 650/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 650, Loss: 0.040302395820617676\n",
            "63/63 [==============================] - 31s 463ms/step - loss: 0.0301\n",
            "Epoch 651/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0233Epoch 651, Loss: 0.014530835673213005\n",
            "63/63 [==============================] - 34s 523ms/step - loss: 0.0232\n",
            "Epoch 652/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 652, Loss: 0.03091014362871647\n",
            "63/63 [==============================] - 31s 471ms/step - loss: 0.0292\n",
            "Epoch 653/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 653, Loss: 0.07923459261655807\n",
            "63/63 [==============================] - 32s 477ms/step - loss: 0.0298\n",
            "Epoch 654/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 654, Loss: 0.016305740922689438\n",
            "63/63 [==============================] - 33s 506ms/step - loss: 0.0293\n",
            "Epoch 655/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 655, Loss: 0.018702790141105652\n",
            "63/63 [==============================] - 33s 495ms/step - loss: 0.0283\n",
            "Epoch 656/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0254Epoch 656, Loss: 0.014319602400064468\n",
            "63/63 [==============================] - 26s 380ms/step - loss: 0.0253\n",
            "Epoch 657/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 657, Loss: 0.03286516293883324\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0266\n",
            "Epoch 658/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 658, Loss: 0.014793380163609982\n",
            "63/63 [==============================] - 27s 423ms/step - loss: 0.0288\n",
            "Epoch 659/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0300Epoch 659, Loss: 0.005540183745324612\n",
            "63/63 [==============================] - 30s 449ms/step - loss: 0.0296\n",
            "Epoch 660/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 660, Loss: 0.04066012427210808\n",
            "63/63 [==============================] - 29s 440ms/step - loss: 0.0297\n",
            "Epoch 661/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 661, Loss: 0.03591432422399521\n",
            "63/63 [==============================] - 30s 457ms/step - loss: 0.0292\n",
            "Epoch 662/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 662, Loss: 0.01150787714868784\n",
            "63/63 [==============================] - 30s 459ms/step - loss: 0.0291\n",
            "Epoch 663/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 663, Loss: 0.01771843060851097\n",
            "63/63 [==============================] - 32s 486ms/step - loss: 0.0266\n",
            "Epoch 664/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 664, Loss: 0.05235133320093155\n",
            "63/63 [==============================] - 30s 457ms/step - loss: 0.0290\n",
            "Epoch 665/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 665, Loss: 0.053284306079149246\n",
            "63/63 [==============================] - 30s 444ms/step - loss: 0.0285\n",
            "Epoch 666/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 666, Loss: 0.0076143089681863785\n",
            "63/63 [==============================] - 31s 468ms/step - loss: 0.0309\n",
            "Epoch 667/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 667, Loss: 0.03050326555967331\n",
            "63/63 [==============================] - 30s 449ms/step - loss: 0.0266\n",
            "Epoch 668/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0266Epoch 668, Loss: 0.011346124112606049\n",
            "63/63 [==============================] - 29s 446ms/step - loss: 0.0264\n",
            "Epoch 669/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 669, Loss: 0.04712299257516861\n",
            "63/63 [==============================] - 30s 456ms/step - loss: 0.0266\n",
            "Epoch 670/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 670, Loss: 0.022269688546657562\n",
            "63/63 [==============================] - 32s 482ms/step - loss: 0.0288\n",
            "Epoch 671/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0250Epoch 671, Loss: 0.007640622556209564\n",
            "63/63 [==============================] - 31s 469ms/step - loss: 0.0247\n",
            "Epoch 672/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 672, Loss: 0.0320272222161293\n",
            "63/63 [==============================] - 29s 437ms/step - loss: 0.0274\n",
            "Epoch 673/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 673, Loss: 0.015350516885519028\n",
            "63/63 [==============================] - 30s 455ms/step - loss: 0.0295\n",
            "Epoch 674/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0280Epoch 674, Loss: 0.03659023344516754\n",
            "63/63 [==============================] - 30s 447ms/step - loss: 0.0281\n",
            "Epoch 675/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0262Epoch 675, Loss: 0.030435308814048767\n",
            "63/63 [==============================] - 30s 461ms/step - loss: 0.0263\n",
            "Epoch 676/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 676, Loss: 0.026644881814718246\n",
            "63/63 [==============================] - 30s 454ms/step - loss: 0.0268\n",
            "Epoch 677/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 677, Loss: 0.023790225386619568\n",
            "63/63 [==============================] - 31s 467ms/step - loss: 0.0291\n",
            "Epoch 678/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 678, Loss: 0.021714311093091965\n",
            "63/63 [==============================] - 32s 487ms/step - loss: 0.0264\n",
            "Epoch 679/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0259Epoch 679, Loss: 0.0354037880897522\n",
            "63/63 [==============================] - 31s 464ms/step - loss: 0.0261\n",
            "Epoch 680/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 680, Loss: 0.021617179736495018\n",
            "63/63 [==============================] - 30s 455ms/step - loss: 0.0268\n",
            "Epoch 681/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 681, Loss: 0.011864572763442993\n",
            "63/63 [==============================] - 30s 459ms/step - loss: 0.0289\n",
            "Epoch 682/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 682, Loss: 0.06793524324893951\n",
            "63/63 [==============================] - 29s 442ms/step - loss: 0.0308\n",
            "Epoch 683/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 683, Loss: 0.020941216498613358\n",
            "63/63 [==============================] - 29s 432ms/step - loss: 0.0281\n",
            "Epoch 684/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 684, Loss: 0.052506059408187866\n",
            "63/63 [==============================] - 29s 440ms/step - loss: 0.0299\n",
            "Epoch 685/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 685, Loss: 0.014544583857059479\n",
            "63/63 [==============================] - 30s 448ms/step - loss: 0.0279\n",
            "Epoch 686/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 686, Loss: 0.012389887124300003\n",
            "63/63 [==============================] - 29s 445ms/step - loss: 0.0266\n",
            "Epoch 687/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0244Epoch 687, Loss: 0.008655612356960773\n",
            "63/63 [==============================] - 30s 456ms/step - loss: 0.0241\n",
            "Epoch 688/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 688, Loss: 0.043259475380182266\n",
            "63/63 [==============================] - 30s 454ms/step - loss: 0.0318\n",
            "Epoch 689/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 689, Loss: 0.006266665644943714\n",
            "63/63 [==============================] - 32s 488ms/step - loss: 0.0260\n",
            "Epoch 690/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 690, Loss: 0.014109364710748196\n",
            "63/63 [==============================] - 32s 485ms/step - loss: 0.0290\n",
            "Epoch 691/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 691, Loss: 0.018906250596046448\n",
            "63/63 [==============================] - 30s 458ms/step - loss: 0.0293\n",
            "Epoch 692/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 692, Loss: 0.0187411829829216\n",
            "63/63 [==============================] - 30s 457ms/step - loss: 0.0312\n",
            "Epoch 693/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 693, Loss: 0.017274830490350723\n",
            "63/63 [==============================] - 30s 462ms/step - loss: 0.0311\n",
            "Epoch 694/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0243Epoch 694, Loss: 0.03060947358608246\n",
            "63/63 [==============================] - 29s 440ms/step - loss: 0.0244\n",
            "Epoch 695/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 695, Loss: 0.011920234188437462\n",
            "63/63 [==============================] - 29s 447ms/step - loss: 0.0269\n",
            "Epoch 696/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 696, Loss: 0.00620354525744915\n",
            "63/63 [==============================] - 29s 434ms/step - loss: 0.0278\n",
            "Epoch 697/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 697, Loss: 0.012159505859017372\n",
            "63/63 [==============================] - 31s 472ms/step - loss: 0.0299\n",
            "Epoch 698/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0278Epoch 698, Loss: 0.009448107331991196\n",
            "63/63 [==============================] - 30s 460ms/step - loss: 0.0275\n",
            "Epoch 699/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 699, Loss: 0.03049410507082939\n",
            "63/63 [==============================] - 31s 468ms/step - loss: 0.0270\n",
            "Epoch 700/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 700, Loss: 0.04402710869908333\n",
            "63/63 [==============================] - 30s 466ms/step - loss: 0.0310\n",
            "Epoch 701/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0268Epoch 701, Loss: 0.01885753683745861\n",
            "63/63 [==============================] - 32s 477ms/step - loss: 0.0267\n",
            "Epoch 702/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 702, Loss: 0.0096622034907341\n",
            "63/63 [==============================] - 29s 444ms/step - loss: 0.0284\n",
            "Epoch 703/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0239Epoch 703, Loss: 0.009080920368432999\n",
            "63/63 [==============================] - 29s 448ms/step - loss: 0.0237\n",
            "Epoch 704/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0255Epoch 704, Loss: 0.03811950981616974\n",
            "63/63 [==============================] - 30s 449ms/step - loss: 0.0257\n",
            "Epoch 705/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 705, Loss: 0.01503389049321413\n",
            "63/63 [==============================] - 30s 463ms/step - loss: 0.0283\n",
            "Epoch 706/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0263Epoch 706, Loss: 0.014686308801174164\n",
            "63/63 [==============================] - 30s 461ms/step - loss: 0.0261\n",
            "Epoch 707/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 707, Loss: 0.01179184764623642\n",
            "63/63 [==============================] - 30s 451ms/step - loss: 0.0281\n",
            "Epoch 708/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0319Epoch 708, Loss: 0.011081458069384098\n",
            "63/63 [==============================] - 31s 464ms/step - loss: 0.0315\n",
            "Epoch 709/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0305Epoch 709, Loss: 0.05435648933053017\n",
            "63/63 [==============================] - 32s 488ms/step - loss: 0.0308\n",
            "Epoch 710/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 710, Loss: 0.025960519909858704\n",
            "63/63 [==============================] - 31s 465ms/step - loss: 0.0271\n",
            "Epoch 711/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 711, Loss: 0.017568113282322884\n",
            "63/63 [==============================] - 30s 452ms/step - loss: 0.0278\n",
            "Epoch 712/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0310Epoch 712, Loss: 0.0435958094894886\n",
            "63/63 [==============================] - 29s 437ms/step - loss: 0.0312\n",
            "Epoch 713/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 713, Loss: 0.05047229304909706\n",
            "63/63 [==============================] - 30s 446ms/step - loss: 0.0301\n",
            "Epoch 714/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0284Epoch 714, Loss: 0.013457706198096275\n",
            "63/63 [==============================] - 27s 401ms/step - loss: 0.0282\n",
            "Epoch 715/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 715, Loss: 0.013103683479130268\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0295\n",
            "Epoch 716/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0244Epoch 716, Loss: 0.06621051579713821\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0250\n",
            "Epoch 717/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0244Epoch 717, Loss: 0.030728964135050774\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0245\n",
            "Epoch 718/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0306Epoch 718, Loss: 0.016356617212295532\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0304\n",
            "Epoch 719/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 719, Loss: 0.04762638360261917\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0291\n",
            "Epoch 720/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 720, Loss: 0.021009694784879684\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0276\n",
            "Epoch 721/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0245Epoch 721, Loss: 0.012038134038448334\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0243\n",
            "Epoch 722/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 722, Loss: 0.01656043715775013\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0264\n",
            "Epoch 723/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 723, Loss: 0.021398281678557396\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0269\n",
            "Epoch 724/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 724, Loss: 0.05026575177907944\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0288\n",
            "Epoch 725/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 725, Loss: 0.02284356951713562\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0287\n",
            "Epoch 726/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 726, Loss: 0.024094125255942345\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0286\n",
            "Epoch 727/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0272Epoch 727, Loss: 0.022232824936509132\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0272\n",
            "Epoch 728/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0266Epoch 728, Loss: 0.014035254716873169\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0264\n",
            "Epoch 729/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0314Epoch 729, Loss: 0.03256663307547569\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0314\n",
            "Epoch 730/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0257Epoch 730, Loss: 0.038165733218193054\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0259\n",
            "Epoch 731/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 731, Loss: 0.01702767238020897\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0288\n",
            "Epoch 732/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 732, Loss: 0.018896542489528656\n",
            "63/63 [==============================] - 24s 375ms/step - loss: 0.0270\n",
            "Epoch 733/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 733, Loss: 0.048546090722084045\n",
            "63/63 [==============================] - 24s 374ms/step - loss: 0.0319\n",
            "Epoch 734/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 734, Loss: 0.06100287660956383\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0284\n",
            "Epoch 735/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0288Epoch 735, Loss: 0.023700041696429253\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0288\n",
            "Epoch 736/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 736, Loss: 0.014090518467128277\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0300\n",
            "Epoch 737/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0316Epoch 737, Loss: 0.04316873103380203\n",
            "63/63 [==============================] - 29s 454ms/step - loss: 0.0318\n",
            "Epoch 738/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0250Epoch 738, Loss: 0.011906708590686321\n",
            "63/63 [==============================] - 34s 516ms/step - loss: 0.0248\n",
            "Epoch 739/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0274Epoch 739, Loss: 0.011960857547819614\n",
            "63/63 [==============================] - 36s 546ms/step - loss: 0.0272\n",
            "Epoch 740/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0299Epoch 740, Loss: 0.02726779505610466\n",
            "63/63 [==============================] - 28s 411ms/step - loss: 0.0299\n",
            "Epoch 741/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0282Epoch 741, Loss: 0.06429719179868698\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0287\n",
            "Epoch 742/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0254Epoch 742, Loss: 0.035160213708877563\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0255\n",
            "Epoch 743/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 743, Loss: 0.05584121495485306\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0294\n",
            "Epoch 744/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 744, Loss: 0.019720539450645447\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0293\n",
            "Epoch 745/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 745, Loss: 0.009643640369176865\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0288\n",
            "Epoch 746/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0285Epoch 746, Loss: 0.021685313433408737\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0284\n",
            "Epoch 747/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0277Epoch 747, Loss: 0.018011894077062607\n",
            "63/63 [==============================] - 28s 435ms/step - loss: 0.0276\n",
            "Epoch 748/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0296Epoch 748, Loss: 0.024476507678627968\n",
            "63/63 [==============================] - 32s 482ms/step - loss: 0.0296\n",
            "Epoch 749/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 749, Loss: 0.023841749876737595\n",
            "63/63 [==============================] - 33s 510ms/step - loss: 0.0288\n",
            "Epoch 750/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0291Epoch 750, Loss: 0.008235195651650429\n",
            "63/63 [==============================] - 27s 406ms/step - loss: 0.0287\n",
            "Epoch 751/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 751, Loss: 0.02761334739625454\n",
            "63/63 [==============================] - 34s 504ms/step - loss: 0.0270\n",
            "Epoch 752/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0230Epoch 752, Loss: 0.009756912477314472\n",
            "63/63 [==============================] - 35s 532ms/step - loss: 0.0228\n",
            "Epoch 753/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 753, Loss: 0.020150724798440933\n",
            "63/63 [==============================] - 37s 563ms/step - loss: 0.0297\n",
            "Epoch 754/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0255Epoch 754, Loss: 0.0380469411611557\n",
            "63/63 [==============================] - 35s 535ms/step - loss: 0.0257\n",
            "Epoch 755/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 755, Loss: 0.02026374079287052\n",
            "63/63 [==============================] - 35s 536ms/step - loss: 0.0274\n",
            "Epoch 756/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 756, Loss: 0.0036852930206805468\n",
            "63/63 [==============================] - 38s 571ms/step - loss: 0.0262\n",
            "Epoch 757/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0251Epoch 757, Loss: 0.04482130706310272\n",
            "63/63 [==============================] - 36s 540ms/step - loss: 0.0254\n",
            "Epoch 758/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0293Epoch 758, Loss: 0.025304727256298065\n",
            "63/63 [==============================] - 37s 558ms/step - loss: 0.0292\n",
            "Epoch 759/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0302Epoch 759, Loss: 0.13847412168979645\n",
            "63/63 [==============================] - 35s 537ms/step - loss: 0.0319\n",
            "Epoch 760/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 760, Loss: 0.02198116108775139\n",
            "63/63 [==============================] - 40s 601ms/step - loss: 0.0263\n",
            "Epoch 761/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 761, Loss: 0.020193684846162796\n",
            "63/63 [==============================] - 26s 384ms/step - loss: 0.0291\n",
            "Epoch 762/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0317Epoch 762, Loss: 0.019979584962129593\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0315\n",
            "Epoch 763/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 763, Loss: 0.0075135803781449795\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0264\n",
            "Epoch 764/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0297Epoch 764, Loss: 0.03420519456267357\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0297\n",
            "Epoch 765/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0275Epoch 765, Loss: 0.02068757638335228\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0274\n",
            "Epoch 766/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0295Epoch 766, Loss: 0.01744496077299118\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0293\n",
            "Epoch 767/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0279Epoch 767, Loss: 0.03248339146375656\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0280\n",
            "Epoch 768/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0301Epoch 768, Loss: 0.020538533106446266\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0299\n",
            "Epoch 769/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0254Epoch 769, Loss: 0.04606449231505394\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0257\n",
            "Epoch 770/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0298Epoch 770, Loss: 0.05227862298488617\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0301\n",
            "Epoch 771/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 771, Loss: 0.019711840897798538\n",
            "63/63 [==============================] - 23s 366ms/step - loss: 0.0284\n",
            "Epoch 772/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0292Epoch 772, Loss: 0.01208951324224472\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0289\n",
            "Epoch 773/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 773, Loss: 0.01390586793422699\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0288\n",
            "Epoch 774/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0278Epoch 774, Loss: 0.046531639993190765\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0281\n",
            "Epoch 775/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 775, Loss: 0.01316059473901987\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0288\n",
            "Epoch 776/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 776, Loss: 0.04930007457733154\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0289\n",
            "Epoch 777/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0308Epoch 777, Loss: 0.01602325215935707\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0306\n",
            "Epoch 778/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0254Epoch 778, Loss: 0.036878421902656555\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0256\n",
            "Epoch 779/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0270Epoch 779, Loss: 0.015751797705888748\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0268\n",
            "Epoch 780/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 780, Loss: 0.0281448345631361\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0271\n",
            "Epoch 781/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 781, Loss: 0.007498312741518021\n",
            "63/63 [==============================] - 23s 366ms/step - loss: 0.0283\n",
            "Epoch 782/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0258Epoch 782, Loss: 0.013986730948090553\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0256\n",
            "Epoch 783/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0271Epoch 783, Loss: 0.057084519416093826\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0276\n",
            "Epoch 784/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0313Epoch 784, Loss: 0.010706141591072083\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0310\n",
            "Epoch 785/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0328Epoch 785, Loss: 0.06399184465408325\n",
            "63/63 [==============================] - 23s 367ms/step - loss: 0.0333\n",
            "Epoch 786/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 786, Loss: 0.04023337736725807\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 0.0271\n",
            "Epoch 787/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0252Epoch 787, Loss: 0.04385294392704964\n",
            "63/63 [==============================] - 23s 369ms/step - loss: 0.0255\n",
            "Epoch 788/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 788, Loss: 0.0327279195189476\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0270\n",
            "Epoch 789/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 789, Loss: 0.039617620408535004\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0288\n",
            "Epoch 790/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0265Epoch 790, Loss: 0.042255766689777374\n",
            "63/63 [==============================] - 24s 373ms/step - loss: 0.0267\n",
            "Epoch 791/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0289Epoch 791, Loss: 0.03858470544219017\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0290\n",
            "Epoch 792/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0264Epoch 792, Loss: 0.03541005402803421\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0266\n",
            "Epoch 793/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0281Epoch 793, Loss: 0.05518316105008125\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0286\n",
            "Epoch 794/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0269Epoch 794, Loss: 0.05410486459732056\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0273\n",
            "Epoch 795/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0235Epoch 795, Loss: 0.03490542247891426\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0237\n",
            "Epoch 796/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0286Epoch 796, Loss: 0.030447715893387794\n",
            "63/63 [==============================] - 24s 371ms/step - loss: 0.0286\n",
            "Epoch 797/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0259Epoch 797, Loss: 0.014151998795568943\n",
            "63/63 [==============================] - 24s 370ms/step - loss: 0.0257\n",
            "Epoch 798/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0267Epoch 798, Loss: 0.008570252917706966\n",
            "63/63 [==============================] - 24s 369ms/step - loss: 0.0264\n",
            "Epoch 799/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0290Epoch 799, Loss: 0.029275748878717422\n",
            "63/63 [==============================] - 24s 368ms/step - loss: 0.0291\n",
            "Epoch 800/800\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.0294Epoch 800, Loss: 0.04146328940987587\n",
            "63/63 [==============================] - 24s 372ms/step - loss: 0.0296\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x267317d2fa0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "class DiffusionModel(keras.Model):\n",
        "    def __init__(self, network, ema_network, timesteps, gdf_util, ema=0.999):\n",
        "        super().__init__()\n",
        "        self.network = network\n",
        "        self.ema_network = ema_network\n",
        "        self.timesteps = timesteps\n",
        "        self.gdf_util = gdf_util\n",
        "        self.ema = ema\n",
        "\n",
        "    def train_step(self, images):\n",
        "        # 1. Get the batch size\n",
        "        batch_size = tf.shape(images)[0]\n",
        "\n",
        "        # 2. Sample timesteps uniformly\n",
        "        t = tf.random.uniform(\n",
        "            minval=0, maxval=self.timesteps, shape=(batch_size,), dtype=tf.int64\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 3. Sample random noise to be added to the images in the batch\n",
        "            noise = tf.random.normal(shape=tf.shape(images), dtype=images.dtype)\n",
        "\n",
        "            # 4. Diffuse the images with noise\n",
        "            images_t = self.gdf_util.q_sample(images, t, noise)\n",
        "\n",
        "            # 5. Pass the diffused images and time steps to the network\n",
        "            pred_noise = self.network([images_t, t], training=True)\n",
        "\n",
        "            # 6. Calculate the loss\n",
        "            loss = self.loss(noise, pred_noise)\n",
        "\n",
        "        # 7. Get the gradients\n",
        "        gradients = tape.gradient(loss, self.network.trainable_weights)\n",
        "\n",
        "        # 8. Update the weights of the network\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
        "\n",
        "        # 9. Updates the weight values for the network with EMA weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)\n",
        "\n",
        "        # 10. Return loss values\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def generate_images(self, num_images=16):\n",
        "        # 1. Randomly sample noise (starting point for reverse process)\n",
        "        samples = tf.random.normal(\n",
        "            shape=(num_images, img_size, img_size, img_channels), dtype=tf.float32\n",
        "        )\n",
        "        # 2. Sample from the model iteratively\n",
        "        for t in reversed(range(0, self.timesteps)):\n",
        "            tt = tf.cast(tf.fill(num_images, t), dtype=tf.int64)\n",
        "            pred_noise = self.ema_network.predict(\n",
        "                [samples, tt], verbose=0, batch_size=num_images\n",
        "            )\n",
        "            samples = self.gdf_util.p_sample(\n",
        "                pred_noise, samples, tt, clip_denoised=True\n",
        "            )\n",
        "        # 3. Return generated samples\n",
        "        return samples\n",
        "\n",
        "    def plot_images(\n",
        "        self, epoch=None, logs=None, num_rows=0, num_cols=0, figsize=(12, 5)\n",
        "    ):\n",
        "        \"\"\"Utility to plot images using the diffusion model during training.\"\"\"\n",
        "        generated_samples = self.generate_images(num_images=num_rows * num_cols)\n",
        "        generated_samples = (\n",
        "            tf.clip_by_value(generated_samples * 127.5 + 127.5, 0.0, 255.0)\n",
        "            .numpy()\n",
        "            .astype(np.uint8)\n",
        "        )\n",
        "\n",
        "        _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "        for i, image in enumerate(generated_samples):\n",
        "            if num_rows == 1:\n",
        "                ax[i].imshow(image)\n",
        "                ax[i].axis(\"off\")\n",
        "            else:\n",
        "                ax[i // num_cols, i % num_cols].imshow(image)\n",
        "                ax[i // num_cols, i % num_cols].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Build the unet model\n",
        "network = build_model(\n",
        "    img_size=img_size,\n",
        "    img_channels=img_channels,\n",
        "    widths=widths,\n",
        "    has_attention=has_attention,\n",
        "    num_res_blocks=num_res_blocks,\n",
        "    norm_groups=norm_groups,\n",
        "    activation_fn=keras.activations.swish,\n",
        ")\n",
        "ema_network = build_model(\n",
        "    img_size=img_size,\n",
        "    img_channels=img_channels,\n",
        "    widths=widths,\n",
        "    has_attention=has_attention,\n",
        "    num_res_blocks=num_res_blocks,\n",
        "    norm_groups=norm_groups,\n",
        "    activation_fn=keras.activations.swish,\n",
        ")\n",
        "ema_network.set_weights(network.get_weights())  # Initially the weights are the same\n",
        "\n",
        "# Get an instance of the Gaussian Diffusion utilities\n",
        "gdf_util = GaussianDiffusion(timesteps=total_timesteps)\n",
        "\n",
        "# Get the model\n",
        "model = DiffusionModel(\n",
        "    network=network,\n",
        "    ema_network=ema_network,\n",
        "    gdf_util=gdf_util,\n",
        "    timesteps=total_timesteps,\n",
        ")\n",
        "\n",
        "# Define a custom callback to store loss values\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.losses.append(logs[\"loss\"])  # Store loss at the end of each epoch\n",
        "        print(f\"Epoch {epoch+1}, Loss: {logs['loss']}\")  # Print loss for monitoring\n",
        "\n",
        "# Initialize the loss history callback\n",
        "loss_history = LossHistory()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=keras.losses.MeanSquaredError(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        ")\n",
        "\n",
        "# Train the model with the custom callback\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[loss_history],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the loss curvea\n",
        "plt.plot(range(1, num_epochs + 1), loss_history.losses, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load li m hnh\n",
        "# model = keras.models.load_model(\"my_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptyFIlqTgkwV"
      },
      "source": [
        "## Results\n",
        "\n",
        "We trained this model for 800 epochs on a V100 GPU,\n",
        "and each epoch took almost 8 seconds to finish. We load those weights\n",
        "here, and we generate a few samples starting from pure noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yryvr3QlgkwV"
      },
      "outputs": [],
      "source": [
        "# !curl -LO https://github.com/AakashKumarNain/ddpms/releases/download/v3.0.0/checkpoints.zip\n",
        "# !unzip -qq checkpoints.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#if not load weight\n",
        "model.plot_images(num_rows=2, num_cols=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# file_path = \"my_model_weights.h5\"\n",
        "# if os.path.exists(file_path):\n",
        "#     os.remove(file_path)\n",
        "\n",
        "# model.save_weights(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'DiffusionModel' object has no attribute 'loss'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the model by calling it with a dummy image\u001b[39;00m\n\u001b[0;32m     10\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, img_size, img_size, img_channels))  \u001b[38;5;66;03m# 1 sample\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass to initialize variables\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load the weights\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model_load\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_model_weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mDiffusionModel.train_step\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     27\u001b[0m     pred_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork([images_t, t], training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# 6. Calculate the loss\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m(noise, pred_noise)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 7. Get the gradients\u001b[39;00m\n\u001b[0;32m     33\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mtrainable_weights)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'DiffusionModel' object has no attribute 'loss'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Recreate the model\n",
        "model_load = DiffusionModel(\n",
        "    network=network,\n",
        "    ema_network=ema_network,\n",
        "    gdf_util=gdf_util,\n",
        "    timesteps=total_timesteps,\n",
        ")\n",
        "\n",
        "# Initialize the model by calling it with a dummy image\n",
        "dummy_input = tf.random.normal(shape=(1, img_size, img_size, img_channels))  # 1 sample\n",
        "_ = model_load.train_step(dummy_input)  # Forward pass to initialize variables\n",
        "\n",
        "# Load the weights\n",
        "model_load.load_weights(\"my_model_weights.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZDTs65AgkwV"
      },
      "outputs": [],
      "source": [
        "# # Load the model weights\n",
        "# model.ema_network.load_weights(\"./checkpoints_1/diffusion_model_checkpoint\")\n",
        "\n",
        "# # Generate and plot some samples\n",
        "# model.plot_images(num_rows=2, num_cols=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYbIFtx6gkwV"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We successfully implemented and trained a diffusion model exactly in the same\n",
        "fashion as implemented by the authors of the DDPMs paper. You can find the\n",
        "original implementation [here](https://github.com/hojonathanho/diffusion).\n",
        "\n",
        "There are a few things that you can try to improve the model:\n",
        "\n",
        "1. Increasing the width of each block. A bigger model can learn to denoise\n",
        "in fewer epochs, though you may have to take care of overfitting.\n",
        "\n",
        "2. We implemented the linear schedule for variance scheduling. You can implement\n",
        "other schemes like cosine scheduling and compare the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4qSUB4MgkwV"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n",
        "2. [Author's implementation](https://github.com/hojonathanho/diffusion)\n",
        "3. [A deep dive into DDPMs](https://magic-with-latents.github.io/latent/posts/ddpms/part3/)\n",
        "4. [Denoising Diffusion Implicit Models](https://keras.io/examples/generative/ddim/)\n",
        "5. [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n",
        "6. [AIAIART](https://www.youtube.com/watch?v=XTs7M6TSK9I&t=14s)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ddpm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "GPU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
